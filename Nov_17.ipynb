{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "with open('train_features_0.csv', 'r') as f, open('train_features_1.csv', 'r')  as f1 :  # I split the training feature data into two files for the sake of meeting github's file size limit\n",
    "    content = f.read()\n",
    "    content1 = f1.read()\n",
    "    \n",
    "    obj = StringIO(content + content1)\n",
    "    \n",
    "train_feats_df = pd.read_csv(obj)\n",
    "train_target_df = pd.read_csv(\"train_targets_scored.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "clf_dict = {} # A dictionary for saving the individual models\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_feats_df.values, train_target_df.values, test_size=0.2,)  # Train-test split. One fold for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['id_92898f216', 'trt_cp', 48, ..., -0.5912, 0.9209, -0.0457],\n",
       "       ['id_faf092875', 'trt_cp', 72, ..., 1.026, -0.9019, -0.9099],\n",
       "       ['id_a7d37d9c7', 'ctl_vehicle', 24, ..., 0.9264, 0.8839, 0.3978],\n",
       "       ...,\n",
       "       ['id_a23c124dd', 'trt_cp', 24, ..., 0.3596, -0.8082, -1.886],\n",
       "       ['id_7eba18346', 'trt_cp', 72, ..., 0.9712, -1.13, 1.19],\n",
       "       ['id_07f7bedce', 'trt_cp', 72, ..., -1.025, -0.7848, -0.8253]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['id_92898f216', 0, 0, ..., 0, 0, 0],\n",
       "       ['id_faf092875', 0, 0, ..., 0, 0, 0],\n",
       "       ['id_a7d37d9c7', 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       ['id_a23c124dd', 0, 0, ..., 0, 0, 0],\n",
       "       ['id_7eba18346', 0, 0, ..., 0, 0, 0],\n",
       "       ['id_07f7bedce', 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [27:58<00:00, 279.70s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "for combination in tqdm([(24, 'D1'), (48, 'D1'), (72, 'D1'), (24, 'D2'), (48, 'D2'), (72, 'D2')]):  # Each particular combination will have their own classifier\n",
    "    label_clfs_container = []  # A list to keep the classifier\n",
    "    lable_feats_container = []  # A list to keep the feature importance values\n",
    "    for i, label in enumerate(train_target_df.iloc[:,1:].columns):  # Train each label with their own classifier.\n",
    "        \n",
    "    \n",
    "        ind = (X_train[:, 2] == combination[0])  & (X_train[:, 3] == combination[1]) &  (X_train[:, 1] == 'trt_cp')  # Obtaining the training index based on combination\n",
    "\n",
    "\n",
    "\n",
    "        feature_mat = X_train[ind][:, 4:]\n",
    "\n",
    "        target_vector = y_train[ind][:, i + 1]\n",
    "        \n",
    "        # Feature selection\n",
    "        RF_clf = RandomForestClassifier(max_depth=100, random_state=0,n_jobs = 20)\n",
    "        RF_clf.fit(feature_mat.astype(float), target_vector.astype(int))\n",
    "        selected_features =  RF_clf.feature_importances_\n",
    "        feature_mat = feature_mat[:, selected_features != 0]  # I actually found that many feature importance are zero... \n",
    "                                                              # So I only select those that have non-zero feature importance.\n",
    "\n",
    "        lable_feats_container.append(selected_features)\n",
    "        \n",
    "        if (target_vector == 1).any():  # If and only if the the FILTERED target vector contains any activation, then proceed with fitting a classifier. See else below.\n",
    "            \n",
    "            ############### Dealing with class inbalance by over sampling the activated samples ##########################\n",
    "            assert  (target_vector == 1).sum() < (target_vector == 0).sum()\n",
    "            \n",
    "            \n",
    "            idx_of_label_one = target_vector == 1\n",
    "            idx_of_label_zero = target_vector == 0\n",
    "            \n",
    "            num_sample_0 = (target_vector == 0).sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "            resampled_one_feat, resampled_one_target = \\\n",
    "                     resample(feature_mat[idx_of_label_one] , target_vector[idx_of_label_one], n_samples =num_sample_0)\n",
    "            \n",
    "            final_feat = np.append(feature_mat[idx_of_label_zero], resampled_one_feat, axis = 0)\n",
    "            \n",
    "            final_target = np.append(target_vector[idx_of_label_zero], resampled_one_target, axis = 0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ################# Finally, fit a simple LS model and add it to the class dictionary #########################\n",
    "            clf = LogisticRegression(max_iter =1000)\n",
    "            clf.fit(final_feat.astype('float32'), final_target.astype('int'))\n",
    "            \n",
    "            label_clfs_container.append(clf)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        else:  # Else (if no activiation is found), then add zero as a classifier. \n",
    "            label_clfs_container.append(0)\n",
    "        \n",
    "    clf_dict[combination] = (label_clfs_container, lable_feats_container)\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:34<00:00,  5.82s/it]\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros(y_test[:, 1:].shape)  # Initiazlie a prediction matrix to hold prediction; matrix has the same size as the y_test\n",
    "for combination in tqdm([(24, 'D1'), (48, 'D1'), (72, 'D1'), (24, 'D2'), (48, 'D2'), (72, 'D2')]):  # Iterating over each combiantions, same as what I did before\n",
    "    \n",
    "    temp_result = [ ]\n",
    "    \n",
    "    idx = (X_test[:, 2] == combination[0])  & (X_test[:, 3] == combination[1]) &  (X_test[:, 1] == 'trt_cp')\n",
    "    test_feat_mat = X_test[idx][:, 4:]\n",
    "    for classifier,feature_idx in zip(clf_dict[combination][0], clf_dict[combination][1],):  # Iterating over classifer for EACH label. `feature_idx` contains the feature importance for each classifier and it is used to filter the test features.\n",
    "        \n",
    "        if classifier != 0:  # Proceed if a classifer exist\n",
    "            assert classifier.classes_[0] == 0\n",
    "            \n",
    "            feat_mat = test_feat_mat[:, feature_idx != 0]\n",
    "      \n",
    "\n",
    "            pred_prob = classifier.predict_proba(feat_mat)[:, 1]\n",
    "            \n",
    "            temp_result.append(pred_prob)\n",
    "        else:  # If a classifier is not available, then just append zeros since there is no activiation found for that label.\n",
    "            temp_result.append([0] * sum(idx))\n",
    "            \n",
    "    temp_result = np.array(temp_result).T\n",
    "    \n",
    "    pred[idx] = temp_result  # Assign these values to the prediciotn matrix based on the index\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030326035639150442"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(y_true, y_pred):\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean(axis = 0).mean()\n",
    "\n",
    "pred[pred < 1e-15] = 1e-15  \n",
    "pred[pred == 1] = 1 - 1e-15 \n",
    "\n",
    "evaluation(y_test[:, 1:], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
