{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AOFO8RQzbVro"
   },
   "outputs": [],
   "source": [
    "# Importing Relevant Libraries\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3PtHOK2lbVrs"
   },
   "outputs": [],
   "source": [
    "# Reading Relevant Files\n",
    "\n",
    "# File location and type\n",
    "file_location = \"./lish-moa/train_features.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "train_feats_df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T9zpoz1HbVrs"
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"./lish-moa/train_targets_scored.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "train_targets_df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ME95fqfPbVrs"
   },
   "outputs": [],
   "source": [
    "# Selecting samples with cp_type = 'trt_cp'\n",
    "train_feats_df = train_feats_df.filter(\"cp_type = 'trt_cp'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MekBqfRlbVrs"
   },
   "outputs": [],
   "source": [
    "# Label Encoding cp_time feature (24:0, 48:1, 72:2)\n",
    "\n",
    "# Converting cp_time column to cp_time vector\n",
    "assembler = VectorAssembler(inputCols = ['cp_time'], outputCol = 'cp_time_vector')\n",
    "train_feats_df = assembler.transform(train_feats_df)\n",
    "train_feats_df = train_feats_df.drop('cp_time')\n",
    "\n",
    "mm_scaler = MinMaxScaler(inputCol = 'cp_time_vector', outputCol = 'cp_time_scaled_vector', min = 0, max = 2)\n",
    "train_feats_df = mm_scaler.fit(train_feats_df).transform(train_feats_df)\n",
    "train_feats_df = train_feats_df.drop('cp_time_vector')\n",
    "\n",
    "firstElement=udf(lambda v:float(v[0]),FloatType())\n",
    "train_feats_df =  train_feats_df.withColumn('cp_time',firstElement(train_feats_df['cp_time_scaled_vector'])).drop('cp_time_scaled_vector')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J8wOwGPtbVrs"
   },
   "outputs": [],
   "source": [
    "# One hot encoding cp_dose feature\n",
    "string_indexer = StringIndexer(inputCol = 'cp_dose', outputCol = 'cp_dose_indexed')\n",
    "train_feats_df = string_indexer.fit(train_feats_df).transform(train_feats_df).drop('cp_dose')\n",
    "one_hot_encoder = OneHotEncoder(inputCol = 'cp_dose_indexed', outputCol = 'cp_dose', dropLast = False)\n",
    "train_feats_df  = one_hot_encoder.fit(train_feats_df).transform(train_feats_df).drop('cp_dose_indexed')\n",
    "\n",
    "one_hot_vectors = (train_feats_df.select('cp_dose','sig_id').collect())\n",
    "l = []\n",
    "for i in one_hot_vectors:\n",
    "  a = (list(i[0].toArray()) + [i[1]])\n",
    "  l.append(a)\n",
    "cp_dose_one_hot = spark.createDataFrame(pd.DataFrame(l, columns = ['D1', 'D2', 'sig_id']))\n",
    "\n",
    "train_feats_df = train_feats_df.join(cp_dose_one_hot, train_feats_df.sig_id == cp_dose_one_hot.sig_id).drop(cp_dose_one_hot.sig_id).drop('cp_dose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HW_O8oN0bVrt"
   },
   "outputs": [],
   "source": [
    "target_names = train_targets_df.columns # List to store target names\n",
    "feature_names = train_feats_df.columns # List to store feature names\n",
    "\n",
    "gene_feature_names = [] # List to store gene feature names\n",
    "cell_feature_names = [] # List to store cell feature names\n",
    "for i in range(772):\n",
    "  gene_feature_names.append('g-' + str(i))\n",
    "for i in range(100):\n",
    "  cell_feature_names.append('c-' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fBOEV54RbVrt"
   },
   "outputs": [],
   "source": [
    "# Logloss for the case when all MoAs are 0 for the label\n",
    "def logloss(predictedProb, trueLabel):\n",
    "    loss = (-trueLabel*np.log(predictedProb + 10e-9) - (1-trueLabel)*(np.log(1-predictedProb))).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VdVwGMA4bVrt",
    "outputId": "b18d8414-d57b-44c6-92a2-6443909794bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34923935de85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mrandomForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Random Forest Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mrandomForestModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomForest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Random Forest Model Trained'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomForestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatureImportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "losses =[]\n",
    "fraction = {0: test_size, 1: test_size}\n",
    "loss = 0\n",
    "# Joininig features and targets\n",
    "data = train_feats_df.join(train_targets_df, train_feats_df.sig_id == train_targets_df.sig_id).drop(train_targets_df.sig_id)\n",
    "n = len(train_targets_df.columns) # total number of labels\n",
    "\n",
    "# Looping over every label\n",
    "for i in sc.range(1,n).collect():\n",
    "  label = train_targets_df.columns[i] \n",
    "  # Selecting gene features, cell features and the label     \n",
    "  label_df = data.select(feature_names + [label])\n",
    "  \n",
    "  # Splitting into train and test    \n",
    "  test_df = label_df.sampleBy(label, fractions = fraction, seed = 42)\n",
    "  label_df.createOrReplaceTempView('label_df_table')\n",
    "  test_df.createOrReplaceTempView('test_df_table')\n",
    "\n",
    "  # Removing the samples that went to test_df from train_df     \n",
    "  train_df = spark.sql('select * from label_df_table as t1 where t1.sig_id not in (select t2.sig_id from test_df_table as t2)')\n",
    "  train_df.createOrReplaceTempView('train_df_table')\n",
    "  \n",
    "  one_count_test = test_df.groupBy(label).count().count() # = 2 if has both 1 and 0\n",
    "                                                          # = 1 if has only 0's\n",
    "  \n",
    "  one_count_train = train_df.filter(train_df[label] == 1).count() # Number of train samples with label = 1\n",
    " \n",
    "  \n",
    "   # if test_data has no samples of activated moa but train samples have atleast two activated moa samples, transferring one sample of activated moa from train to test \n",
    "  if one_count_test == 1 and one_count_train >= 2:\n",
    "    \n",
    "    temp_df = train_df.filter(train_df[label] == 1).limit(1)\n",
    "    test_df = test_df.union(temp_df)\n",
    "    test_df.createOrReplaceTempView('test_df_table')\n",
    "    train_df = spark.sql('select * from train_df_table as t1 where t1.sig_id not in (select t2.sig_id from test_df_table as t2)')\n",
    "    \n",
    "  \n",
    "  # Ensuring no data leakage\n",
    "  train_df.createOrReplaceTempView('train_df_table')\n",
    "  intersection = spark.sql('select t1.sig_id from train_df_table as t1 where t1.sig_id in (select t2.sig_id from test_df_table as t2)').count()\n",
    "  assert intersection == 0\n",
    "  \n",
    "  ############### Dealing with class inbalance by over sampling the activated samples ##########################\n",
    "  if one_count_train > 0:\n",
    "      \n",
    "    assert train_df.filter(train_df[label] == 1).count() < train_df.filter(train_df[label] == 0).count()\n",
    "    \n",
    "    temp_df = train_df.filter(train_df[label] == 1).toPandas()\n",
    "    temp_df = spark.createDataFrame(temp_df.sample(n = train_df.groupBy(label).count().orderBy(label).collect()[0][1], replace = True))\n",
    "    train_df = train_df.filter(train_df[label] == 0).union(temp_df)\n",
    "    \n",
    "    X_train = train_df.select(gene_feature_names + cell_feature_names + ['cp_time', 'D1', 'D2', label])\n",
    "    X_train = X_train.orderBy(rand()) #Shuffling X_train\n",
    "    X_test = test_df.select(gene_feature_names + cell_feature_names + ['cp_time', 'D1', 'D2', label])\n",
    "    \n",
    "    #Assembling features\n",
    "    va = VectorAssembler(inputCols = gene_feature_names + cell_feature_names + ['cp_time', 'D1', 'D2'], outputCol = 'features')\n",
    "    X_train  = va.transform(X_train).rdd.repartition(1000).toDF()\n",
    "    \n",
    "    # Training Random Forest for feature selection\n",
    "    randomForest = RandomForestClassifier(labelCol = label)\n",
    "    print('Training Random Forest Model')\n",
    "    randomForestModel = randomForest.fit(X_train)\n",
    "    print('Random Forest Model Trained')\n",
    "    feature_importance = randomForestModel.featureImportances.toArray()\n",
    "    selected_features = [x for x, y in zip(gene_feature_names + cell_feature_names + ['cp_time', 'D1', 'D2'], list(feature_importance != 0)) if y == True] # List of features with imporatnce > 0\n",
    "    va = VectorAssembler(inputCols = selected_features, outputCol = 'features')\n",
    "    \n",
    "    # Feature Selection\n",
    "    X_train = va.transform(X_train.select(selected_features + [label]))\n",
    "    X_test = va.transform(X_test.select(selected_features + [label]))\n",
    "    \n",
    "    # Training Logistic Regression Model\n",
    "    logisticRegression = LogisticRegression(labelCol = label)\n",
    "    print('Training Logistic Regression Model')\n",
    "    logisticRegressionModel = logisticRegression.fit(X_train)\n",
    "    print('Logistic Regression Trained')\n",
    "    pred_df = logisticRegressionModel.transform(X_test)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label, metricName='logLoss')\n",
    "    loss += evaluator.evaluate(pred_df)\n",
    "\n",
    "      \n",
    "    \n",
    "  else:# Else (if no activiation is found), then always predict 0\n",
    "    n = test_df.count()\n",
    "    prob = np.array([0]*n)\n",
    "    true_label = np.array(test_df.select(label).toPandas())[:,0]\n",
    "    loss += logloss(prob, true_label)\n",
    "    \n",
    "\n",
    "  print(f'Loss after {i}: {label} = {loss/(i+1)}')\n",
    "  del X_train, X_test, train_df, test_df  \n",
    "losses.append(loss/(i+1))\n",
    "\n",
    "    \n",
    "  \n",
    "print('Losses', losses)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BR_with_startification(copy).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
