{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark  # Get rid of this in DataBricks\n",
    "findspark.init()  # Get rid of this in DataBricks\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.dynamicAllocation.initialExecutors', '2'), ('spark.executor.memory', '30g'), ('spark.executor.id', 'driver'), ('spark.dynamicAllocation.minExecutors', '2'), ('spark.driver.host', '172.18.40.157'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.app.id', 'local-1607148872400'), ('spark.dynamicAllocation.maxExecutors', '2'), ('spark.rdd.compress', 'True'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.port', '44493'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'proj')]\n"
     ]
    }
   ],
   "source": [
    "config = SparkConf().setAll([('spark.executor.memory', '30g'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "config.setAppName(\"proj\")\n",
    "config.set(\"spark.dynamicAllocation.minExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.maxExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.initialExecutors\", \"2\"); # the number must be between the min and max\n",
    "sc = SparkContext(conf=config)  # start a new sc with the current config\n",
    "spark = SparkSession(sc)\n",
    "sqlc=SQLContext(sc)\n",
    "print(sc.getConf().getAll())  # print all the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('train_feats.csv', header='true', inferSchema= 'true')   # path in HDFS file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = spark.read.csv('train_targets_scored.csv', header='true', inferSchema= 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.join(df_label, on=['sig_id'], how='left_outer')  # Jjoin them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop vechile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.cp_type == 'trt_cp')\n",
    "df = df.drop('cp_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"cp_dose\", outputCol=\"cp_dose_cat\")\n",
    "df1 = indexer.fit(df).transform(df)\n",
    "indexer = StringIndexer(inputCol=\"cp_time\", outputCol=\"cp_time_cat\")\n",
    "df1 = indexer.fit(df1).transform(df1)\n",
    "df1 = df1.drop('cp_dose')\n",
    "df1 = df1.drop('cp_time')\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"cp_time_cat\", \"cp_dose_cat\"],\n",
    "                        outputCols=[\"cp_time_onehot\", \"cp_dose_onehot\"])\n",
    "\n",
    "model = encoder.fit(df1)\n",
    "df1 = model.transform(df1)\n",
    "df1 = df1.withColumn(\"cp_time_cols\", vector_to_array(\"cp_time_onehot\")).select(df1.columns + [col(\"cp_time_cols\")[i] for i in range(2)])\n",
    "df1 = df1.withColumn(\"cp_dose_cols\", vector_to_array(\"cp_dose_onehot\")).select(df1.columns + [col(\"cp_dose_cols\")[i] for i in range(1)])\n",
    "df1 = df1.drop('cp_dose_cat',\n",
    " 'cp_time_cat',\n",
    " 'cp_time_onehot',\n",
    " 'cp_dose_onehot',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering  (credit to Jeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_feature_names = [name for name in df1.columns if 'g-' in name]\n",
    "cell_feature_names =  [name for name in df1.columns if 'c-' in name]\n",
    "\n",
    "df2 = df1.withColumn(\"gene_max\", F.greatest(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"gene_min\", F.least(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_max\", F.greatest(*cell_feature_names))\n",
    "df2 = df2.withColumn(\"cell_min\", F.least(*cell_feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop high correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = gene_feature_names + cell_feature_names  # This came from the previous section\n",
    "vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'feats' )\n",
    "feature_vector = vectorAssembler.transform(df2).select(\"feats\")\n",
    "# pyspark implementation of determining the correlations\n",
    "corr_matrix = Correlation.corr(feature_vector, \"feats\").head()[0]\n",
    "\n",
    "# Convert the correlation desne matrix and apply mask and to get the indicies where high correlations are observed\n",
    "# In here, I convert the correlation matrix to numpy, and then use numpy's mask to obtain the lower traingle of the\n",
    "# matrix. I used numpy becasue pyspark does not have mask.\n",
    "\n",
    "corr_Array = corr_matrix.toArray()\n",
    "masked_corr = np.ma.masked_where(np.triu(np.ones_like(corr_Array, dtype=bool)), corr_Array, copy=True)  \n",
    "idx_high_corr_feats = set(np.argwhere(masked_corr > 0.90)[:,0])  # Set threshold to 90%\n",
    "\n",
    "# Identify the column to drop and then drop it.\n",
    "features_to_drop = np.array(feature_columns)[list(idx_high_corr_feats)].tolist()\n",
    "\n",
    "\n",
    "# Finally\n",
    "\n",
    "df3 = df2.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_names = list(set(df3.columns) - set(df_label.columns))\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = final_feature_names, outputCol = 'feats' )\n",
    "df4 = vectorAssembler.transform(df3).drop(*final_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_label(df, label_name):\n",
    "    \n",
    "    \n",
    "    if df.filter(df[label_name] == 1).count() >= 2:\n",
    "    \n",
    "        temp_df = df.select('feats', label_name)\n",
    "\n",
    "        # stratify split of the dataframe for train-test split\n",
    "        seed = 42\n",
    "        fractions = {1: 0.8, 0: 0.8}\n",
    "        train_df = temp_df.stat.sampleBy(label_name, fractions, seed, )\n",
    "        test_df =  temp_df.subtract(train_df)\n",
    "\n",
    "\n",
    "        # Over/down sampling of the training dataframe, becasue of the imbalanced class label\n",
    "\n",
    "        activation_samples = train_df.filter(train_df[label_name] == 1)\n",
    "        non_activation_samples = train_df.filter(train_df[label_name] == 0)\n",
    "        ratio = activation_samples.count() / non_activation_samples.count()\n",
    "\n",
    "        default_down_sample_ratio = 0.5  # This can be changed, but for now,i am just setting it as 0.5\n",
    "\n",
    "        upsample_ratio = default_down_sample_ratio / ratio\n",
    "\n",
    "        activation_samples_up = activation_samples.sample(True, upsample_ratio, 42)   # Upsample the activation samples\n",
    "\n",
    "        non_activation_samples_down = non_activation_samples.sample(True, default_down_sample_ratio, 42)  # Meanwhile, downsampling the non-activated samples \n",
    "\n",
    "\n",
    "        final_train_df = activation_samples_up.union(non_activation_samples_down).orderBy(F.rand())\n",
    "\n",
    "\n",
    "        # Feature selection using random forrest before using other models\n",
    "\n",
    "        clf = RandomForestClassifier(numTrees=20, maxDepth=5, labelCol = label_name, featuresCol='feats',  seed=42)\n",
    "\n",
    "\n",
    "        model = clf.fit(final_train_df)\n",
    "\n",
    "        feature_importance = model.featureImportances.toArray()\n",
    "        important_feature_idx = feature_importance.argsort()[-int(0.1 * len(feature_importance)):]  # Only get the top 10%, according to \n",
    "                                                                                                    # the feature importance from random forrest classifier\n",
    "\n",
    "\n",
    "\n",
    "        # Now, after getting the index, filter the feature vector based on the above feature importance index\n",
    "\n",
    "        slicer = VectorSlicer(inputCol=\"feats\", outputCol=\"feats_sub\", indices=important_feature_idx)\n",
    "        final_train_df_sub_feats =  slicer.transform(final_train_df).drop('feats')\n",
    "        final_test_df_sub_feats = slicer.transform(test_df).drop('feats')\n",
    "\n",
    "\n",
    "\n",
    "        # Finally, use CV to train the model and get the best parameters\n",
    "\n",
    "        lr = LogisticRegression(maxIter=10, featuresCol='feats_sub',  labelCol = label_name)\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(lr.regParam, [ 0.1, 0.01]) \\\n",
    "                        .addGrid(lr.elasticNetParam, [1,  0])\\\n",
    "                        .build()\n",
    "\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=label_name, metricName='logLoss')\n",
    "        crossval = CrossValidator(estimator=lr,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=3)  \n",
    "\n",
    "        cvModel = crossval.fit(final_train_df_sub_feats)\n",
    "        prediction_df = cvModel.transform(final_test_df_sub_feats)\n",
    "\n",
    "        cvModel.save(f\"./logistics/{label_name}.model\")\n",
    "        prediction_df.write.save(f\"./logistics/{label_name}_prediction_df.parquet\", format=\"parquet\")\n",
    "        final_train_df_sub_feats.write.save(f\"./logistics/{label_name}_train_df.parquet\", format=\"parquet\")\n",
    "\n",
    "        log_loss = evaluator.evaluate(prediction_df)\n",
    "\n",
    "        with open(f\"./logistics/log.log\", 'a') as f:\n",
    "            f.write(str(log_loss) + '\\n') \n",
    "\n",
    "        return (True, log_loss)\n",
    "    else:\n",
    "        return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_individual_label = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratify \n",
    "ref: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark/47672336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/206 [9:05:15<32:42:14, 749.90s/it]"
     ]
    }
   ],
   "source": [
    "with open(f\"./logistics/log.log\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "\n",
    "temp_dict = {}\n",
    "for name in tqdm(df_label.columns[1:]):\n",
    "    temp_dict[name] = train_individual_label(df4,  name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-alpha_reductase_inhibitor',\n",
       " '11-beta-hsd1_inhibitor',\n",
       " 'acat_inhibitor',\n",
       " 'acetylcholine_receptor_agonist',\n",
       " 'acetylcholine_receptor_antagonist',\n",
       " 'acetylcholinesterase_inhibitor',\n",
       " 'adenosine_receptor_agonist',\n",
       " 'adenosine_receptor_antagonist',\n",
       " 'adenylyl_cyclase_activator',\n",
       " 'adrenergic_receptor_agonist',\n",
       " 'adrenergic_receptor_antagonist',\n",
       " 'akt_inhibitor',\n",
       " 'aldehyde_dehydrogenase_inhibitor',\n",
       " 'alk_inhibitor',\n",
       " 'ampk_activator',\n",
       " 'analgesic',\n",
       " 'androgen_receptor_agonist',\n",
       " 'androgen_receptor_antagonist',\n",
       " 'anesthetic_-_local',\n",
       " 'angiogenesis_inhibitor',\n",
       " 'angiotensin_receptor_antagonist',\n",
       " 'anti-inflammatory',\n",
       " 'antiarrhythmic',\n",
       " 'antibiotic',\n",
       " 'anticonvulsant',\n",
       " 'antifungal',\n",
       " 'antihistamine',\n",
       " 'antimalarial',\n",
       " 'antioxidant',\n",
       " 'antiprotozoal',\n",
       " 'antiviral',\n",
       " 'apoptosis_stimulant',\n",
       " 'aromatase_inhibitor',\n",
       " 'atm_kinase_inhibitor',\n",
       " 'atp-sensitive_potassium_channel_antagonist',\n",
       " 'atp_synthase_inhibitor',\n",
       " 'atpase_inhibitor',\n",
       " 'atr_kinase_inhibitor',\n",
       " 'aurora_kinase_inhibitor',\n",
       " 'autotaxin_inhibitor',\n",
       " 'bacterial_30s_ribosomal_subunit_inhibitor',\n",
       " 'bacterial_50s_ribosomal_subunit_inhibitor',\n",
       " 'bacterial_antifolate',\n",
       " 'bacterial_cell_wall_synthesis_inhibitor',\n",
       " 'bacterial_dna_gyrase_inhibitor',\n",
       " 'bacterial_dna_inhibitor',\n",
       " 'bacterial_membrane_integrity_inhibitor',\n",
       " 'bcl_inhibitor',\n",
       " 'bcr-abl_inhibitor',\n",
       " 'benzodiazepine_receptor_agonist',\n",
       " 'beta_amyloid_inhibitor',\n",
       " 'bromodomain_inhibitor',\n",
       " 'btk_inhibitor',\n",
       " 'calcineurin_inhibitor',\n",
       " 'calcium_channel_blocker',\n",
       " 'cannabinoid_receptor_agonist',\n",
       " 'cannabinoid_receptor_antagonist',\n",
       " 'carbonic_anhydrase_inhibitor',\n",
       " 'casein_kinase_inhibitor',\n",
       " 'caspase_activator',\n",
       " 'catechol_o_methyltransferase_inhibitor',\n",
       " 'cc_chemokine_receptor_antagonist',\n",
       " 'cck_receptor_antagonist',\n",
       " 'cdk_inhibitor',\n",
       " 'chelating_agent',\n",
       " 'chk_inhibitor',\n",
       " 'chloride_channel_blocker',\n",
       " 'cholesterol_inhibitor',\n",
       " 'cholinergic_receptor_antagonist',\n",
       " 'coagulation_factor_inhibitor',\n",
       " 'corticosteroid_agonist',\n",
       " 'cyclooxygenase_inhibitor',\n",
       " 'cytochrome_p450_inhibitor',\n",
       " 'dihydrofolate_reductase_inhibitor',\n",
       " 'dipeptidyl_peptidase_inhibitor',\n",
       " 'diuretic',\n",
       " 'dna_alkylating_agent',\n",
       " 'dna_inhibitor',\n",
       " 'dopamine_receptor_agonist',\n",
       " 'dopamine_receptor_antagonist',\n",
       " 'egfr_inhibitor',\n",
       " 'elastase_inhibitor',\n",
       " 'erbb2_inhibitor',\n",
       " 'estrogen_receptor_agonist',\n",
       " 'estrogen_receptor_antagonist',\n",
       " 'faah_inhibitor',\n",
       " 'farnesyltransferase_inhibitor',\n",
       " 'fatty_acid_receptor_agonist',\n",
       " 'fgfr_inhibitor',\n",
       " 'flt3_inhibitor',\n",
       " 'focal_adhesion_kinase_inhibitor',\n",
       " 'free_radical_scavenger',\n",
       " 'fungal_squalene_epoxidase_inhibitor',\n",
       " 'gaba_receptor_agonist',\n",
       " 'gaba_receptor_antagonist',\n",
       " 'gamma_secretase_inhibitor',\n",
       " 'glucocorticoid_receptor_agonist',\n",
       " 'glutamate_inhibitor',\n",
       " 'glutamate_receptor_agonist',\n",
       " 'glutamate_receptor_antagonist',\n",
       " 'gonadotropin_receptor_agonist',\n",
       " 'gsk_inhibitor',\n",
       " 'hcv_inhibitor',\n",
       " 'hdac_inhibitor',\n",
       " 'histamine_receptor_agonist',\n",
       " 'histamine_receptor_antagonist',\n",
       " 'histone_lysine_demethylase_inhibitor',\n",
       " 'histone_lysine_methyltransferase_inhibitor',\n",
       " 'hiv_inhibitor',\n",
       " 'hmgcr_inhibitor',\n",
       " 'hsp_inhibitor',\n",
       " 'igf-1_inhibitor',\n",
       " 'ikk_inhibitor',\n",
       " 'imidazoline_receptor_agonist',\n",
       " 'immunosuppressant',\n",
       " 'insulin_secretagogue',\n",
       " 'insulin_sensitizer',\n",
       " 'integrin_inhibitor',\n",
       " 'jak_inhibitor',\n",
       " 'kit_inhibitor',\n",
       " 'laxative',\n",
       " 'leukotriene_inhibitor',\n",
       " 'leukotriene_receptor_antagonist',\n",
       " 'lipase_inhibitor',\n",
       " 'lipoxygenase_inhibitor',\n",
       " 'lxr_agonist',\n",
       " 'mdm_inhibitor',\n",
       " 'mek_inhibitor',\n",
       " 'membrane_integrity_inhibitor',\n",
       " 'mineralocorticoid_receptor_antagonist',\n",
       " 'monoacylglycerol_lipase_inhibitor',\n",
       " 'monoamine_oxidase_inhibitor',\n",
       " 'monopolar_spindle_1_kinase_inhibitor',\n",
       " 'mtor_inhibitor',\n",
       " 'mucolytic_agent',\n",
       " 'neuropeptide_receptor_antagonist',\n",
       " 'nfkb_inhibitor',\n",
       " 'nicotinic_receptor_agonist',\n",
       " 'nitric_oxide_donor',\n",
       " 'nitric_oxide_production_inhibitor',\n",
       " 'nitric_oxide_synthase_inhibitor',\n",
       " 'norepinephrine_reuptake_inhibitor',\n",
       " 'nrf2_activator',\n",
       " 'opioid_receptor_agonist',\n",
       " 'opioid_receptor_antagonist',\n",
       " 'orexin_receptor_antagonist',\n",
       " 'p38_mapk_inhibitor',\n",
       " 'p-glycoprotein_inhibitor',\n",
       " 'parp_inhibitor',\n",
       " 'pdgfr_inhibitor',\n",
       " 'pdk_inhibitor',\n",
       " 'phosphodiesterase_inhibitor',\n",
       " 'phospholipase_inhibitor',\n",
       " 'pi3k_inhibitor',\n",
       " 'pkc_inhibitor',\n",
       " 'potassium_channel_activator',\n",
       " 'potassium_channel_antagonist',\n",
       " 'ppar_receptor_agonist',\n",
       " 'ppar_receptor_antagonist',\n",
       " 'progesterone_receptor_agonist',\n",
       " 'progesterone_receptor_antagonist',\n",
       " 'prostaglandin_inhibitor',\n",
       " 'prostanoid_receptor_antagonist',\n",
       " 'proteasome_inhibitor',\n",
       " 'protein_kinase_inhibitor',\n",
       " 'protein_phosphatase_inhibitor',\n",
       " 'protein_synthesis_inhibitor',\n",
       " 'protein_tyrosine_kinase_inhibitor',\n",
       " 'radiopaque_medium',\n",
       " 'raf_inhibitor',\n",
       " 'ras_gtpase_inhibitor',\n",
       " 'retinoid_receptor_agonist',\n",
       " 'retinoid_receptor_antagonist',\n",
       " 'rho_associated_kinase_inhibitor',\n",
       " 'ribonucleoside_reductase_inhibitor',\n",
       " 'rna_polymerase_inhibitor',\n",
       " 'serotonin_receptor_agonist',\n",
       " 'serotonin_receptor_antagonist',\n",
       " 'serotonin_reuptake_inhibitor',\n",
       " 'sigma_receptor_agonist',\n",
       " 'sigma_receptor_antagonist',\n",
       " 'smoothened_receptor_antagonist',\n",
       " 'sodium_channel_inhibitor',\n",
       " 'sphingosine_receptor_agonist',\n",
       " 'src_inhibitor',\n",
       " 'steroid',\n",
       " 'syk_inhibitor',\n",
       " 'tachykinin_antagonist',\n",
       " 'tgf-beta_receptor_inhibitor',\n",
       " 'thrombin_inhibitor',\n",
       " 'thymidylate_synthase_inhibitor',\n",
       " 'tlr_agonist',\n",
       " 'tlr_antagonist',\n",
       " 'tnf_inhibitor',\n",
       " 'topoisomerase_inhibitor',\n",
       " 'transient_receptor_potential_channel_antagonist',\n",
       " 'tropomyosin_receptor_kinase_inhibitor',\n",
       " 'trpv_agonist',\n",
       " 'trpv_antagonist',\n",
       " 'tubulin_inhibitor',\n",
       " 'tyrosine_kinase_inhibitor',\n",
       " 'ubiquitin_specific_protease_inhibitor',\n",
       " 'vegfr_inhibitor',\n",
       " 'vitamin_b',\n",
       " 'vitamin_d_receptor_agonist',\n",
       " 'wnt_inhibitor']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5511764976d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Geeks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geeks'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "my_set = {'Geeks', 'for', 'geeks'} \n",
    "  \n",
    "s = list(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cp_type|\n",
      "+-------+\n",
      "| trt_cp|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cp_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'0.8'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "list = [(2147481832,23355149,1),(2147481832,973010692,1),(2147481832,2134870842,1),(2147481832,541023347,1),(2147481832,1682206630,1),(2147481832,1138211459,1),(2147481832,852202566,1),(2147481832,201375938,1),(2147481832,486538879,1),(2147481832,919187908,1),(214748183,919187908,1),(214748183,91187908,1)]\n",
    "df = spark.createDataFrame(list, [\"x1\",\"x2\",\"x3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: bigint, x2: bigint, x3: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2147481832: 0.8, 214748183: 0.8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|        x1|fraction|\n",
      "+----------+--------+\n",
      "|2147481832|     0.8|\n",
      "| 214748183|     0.8|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = features_and_targets.withColumn('target_vector', (vector_to_string(array([features_and_targets[col] for col in target_names])))).select(['sig_id', 'target_vector'])\n",
    "string_indexer = StringIndexer(inputCol = 'target_vector', outputCol = 'target')\n",
    "string_indexer_model = string_indexer.fit(temp_df)\n",
    "temp_df = string_indexer_model.transform(temp_df).drop('target_vector')\n",
    "\n",
    "data = features_and_targets.join(temp_df, features_and_targets.sig_id == temp_df.sig_id, how = 'inner').drop(temp_df.sig_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
