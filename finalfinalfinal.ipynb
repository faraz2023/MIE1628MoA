{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark  # Get rid of this in DataBricks\n",
    "findspark.init()  # Get rid of this in DataBricks\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.executor.memory', '2g'), ('spark.dynamicAllocation.minExecutors', '1'), ('spark.driver.port', '43143'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1607107377906'), ('spark.driver.host', '172.18.40.157'), ('spark.cores.max', '1'), ('spark.dynamicAllocation.maxExecutors', '1'), ('spark.rdd.compress', 'True'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.executor.cores', '1'), ('spark.submit.deployMode', 'client'), ('spark.dynamicAllocation.initialExecutors', '1'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'proj')]\n"
     ]
    }
   ],
   "source": [
    "config = SparkConf().setAll([('spark.executor.memory', '2g'), ('spark.executor.cores', '1'), ('spark.cores.max', '1'), ('spark.driver.memory','8g')])\n",
    "config.setAppName(\"proj\")\n",
    "config.set(\"spark.dynamicAllocation.minExecutors\", \"1\");\n",
    "config.set(\"spark.dynamicAllocation.maxExecutors\", \"1\");\n",
    "config.set(\"spark.dynamicAllocation.initialExecutors\", \"1\"); # the number must be between the min and max\n",
    "sc = SparkContext(conf=config)  # start a new sc with the current config\n",
    "spark = SparkSession(sc)\n",
    "sqlc=SQLContext(sc)\n",
    "print(sc.getConf().getAll())  # print all the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('train_feats.csv', header='true', inferSchema= 'true')   # path in HDFS file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = spark.read.csv('train_targets_scored.csv', header='true', inferSchema= 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.join(df_label, on=['sig_id'], how='left_outer')  # Jjoin them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop vechile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.cp_type == 'trt_cp')\n",
    "df = df.drop('cp_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"cp_dose\", outputCol=\"cp_dose_cat\")\n",
    "df1 = indexer.fit(df).transform(df)\n",
    "indexer = StringIndexer(inputCol=\"cp_time\", outputCol=\"cp_time_cat\")\n",
    "df1 = indexer.fit(df1).transform(df1)\n",
    "df1 = df1.drop('cp_dose')\n",
    "df1 = df1.drop('cp_time')\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"cp_time_cat\", \"cp_dose_cat\"],\n",
    "                        outputCols=[\"cp_time_onehot\", \"cp_dose_onehot\"])\n",
    "\n",
    "model = encoder.fit(df1)\n",
    "df1 = model.transform(df1)\n",
    "df1 = df1.withColumn(\"cp_time_cols\", vector_to_array(\"cp_time_onehot\")).select(df1.columns + [col(\"cp_time_cols\")[i] for i in range(2)])\n",
    "df1 = df1.withColumn(\"cp_dose_cols\", vector_to_array(\"cp_dose_onehot\")).select(df1.columns + [col(\"cp_dose_cols\")[i] for i in range(1)])\n",
    "df1 = df1.drop('cp_dose_cat',\n",
    " 'cp_time_cat',\n",
    " 'cp_time_onehot',\n",
    " 'cp_dose_onehot',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering  (credit to Jeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_feature_names = [name for name in df1.columns if 'g-' in name]\n",
    "cell_feature_names =  [name for name in df1.columns if 'c-' in name]\n",
    "\n",
    "df2 = df1.withColumn(\"gene_max\", F.greatest(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"gene_min\", F.least(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_max\", F.greatest(*cell_feature_names))\n",
    "df2 = df2.withColumn(\"cell_min\", F.least(*cell_feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop high correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = gene_feature_names + cell_feature_names  # This came from the previous section\n",
    "vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'feats' )\n",
    "feature_vector = vectorAssembler.transform(df2).select(\"feats\")\n",
    "# pyspark implementation of determining the correlations\n",
    "corr_matrix = Correlation.corr(feature_vector, \"feats\").head()[0]\n",
    "\n",
    "# Convert the correlation desne matrix and apply mask and to get the indicies where high correlations are observed\n",
    "# In here, I convert the correlation matrix to numpy, and then use numpy's mask to obtain the lower traingle of the\n",
    "# matrix. I used numpy becasue pyspark does not have mask.\n",
    "\n",
    "corr_Array = corr_matrix.toArray()\n",
    "masked_corr = np.ma.masked_where(np.triu(np.ones_like(corr_Array, dtype=bool)), corr_Array, copy=True)  \n",
    "idx_high_corr_feats = set(np.argwhere(masked_corr > 0.90)[:,0])  # Set threshold to 90%\n",
    "\n",
    "# Identify the column to drop and then drop it.\n",
    "features_to_drop = np.array(feature_columns)[list(idx_high_corr_feats)].tolist()\n",
    "\n",
    "\n",
    "# Finally\n",
    "\n",
    "df3 = df2.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_names = list(set(df3.columns) - set(df_label.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = final_feature_names, outputCol = 'feats' )\n",
    "df4 = vectorAssembler.transform(df3).drop(*final_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_label_name =  '5-alpha_reductase_inhibitor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df4.select('feats', individual_label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify \n",
    "# ref: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark/47672336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "fractions = {1: 0.8, 0: 0.8}\n",
    "train_df = temp_df.stat.sampleBy(individual_label_name, fractions, seed, ).cache()\n",
    "test_df =  temp_df.subtract(train_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[feats: vector, 5-alpha_reductase_inhibitor: int]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(True, 1.1, 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21948"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4265"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5511764976d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Geeks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geeks'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "my_set = {'Geeks', 'for', 'geeks'} \n",
    "  \n",
    "s = list(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cp_type|\n",
      "+-------+\n",
      "| trt_cp|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cp_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'0.8'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "list = [(2147481832,23355149,1),(2147481832,973010692,1),(2147481832,2134870842,1),(2147481832,541023347,1),(2147481832,1682206630,1),(2147481832,1138211459,1),(2147481832,852202566,1),(2147481832,201375938,1),(2147481832,486538879,1),(2147481832,919187908,1),(214748183,919187908,1),(214748183,91187908,1)]\n",
    "df = spark.createDataFrame(list, [\"x1\",\"x2\",\"x3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: bigint, x2: bigint, x3: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2147481832: 0.8, 214748183: 0.8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|        x1|fraction|\n",
      "+----------+--------+\n",
      "|2147481832|     0.8|\n",
      "| 214748183|     0.8|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = features_and_targets.withColumn('target_vector', (vector_to_string(array([features_and_targets[col] for col in target_names])))).select(['sig_id', 'target_vector'])\n",
    "string_indexer = StringIndexer(inputCol = 'target_vector', outputCol = 'target')\n",
    "string_indexer_model = string_indexer.fit(temp_df)\n",
    "temp_df = string_indexer_model.transform(temp_df).drop('target_vector')\n",
    "\n",
    "data = features_and_targets.join(temp_df, features_and_targets.sig_id == temp_df.sig_id, how = 'inner').drop(temp_df.sig_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
