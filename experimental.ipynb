{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark  # Get rid of this in DataBricks\n",
    "findspark.init()  # Get rid of this in DataBricks\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.dynamicAllocation.initialExecutors', '2'), ('spark.executor.memory', '30g'), ('spark.app.id', 'local-1607225993012'), ('spark.executor.id', 'driver'), ('spark.dynamicAllocation.minExecutors', '2'), ('spark.driver.host', '172.18.40.157'), ('spark.executor.cores', '4'), ('spark.dynamicAllocation.maxExecutors', '2'), ('spark.rdd.compress', 'True'), ('spark.driver.port', '38447'), ('spark.driver.memory', '8g'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.name', 'proj'), ('spark.cores.max', '6')]\n"
     ]
    }
   ],
   "source": [
    "config = SparkConf().setAll([('spark.executor.memory', '30g'), ('spark.executor.cores', '4'), ('spark.cores.max', '6'), ('spark.driver.memory','8g')])\n",
    "config.setAppName(\"proj\")\n",
    "config.set(\"spark.dynamicAllocation.minExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.maxExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.initialExecutors\", \"2\"); # the number must be between the min and max\n",
    "\n",
    "# config.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "sc = SparkContext(conf=config)  # start a new sc with the current config\n",
    "spark = SparkSession(sc)\n",
    "sqlc=SQLContext(sc)\n",
    "print(sc.getConf().getAll())  # print all the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('train_feats.csv', header='true', inferSchema= 'true')   # path in HDFS file system\n",
    "df_label = spark.read.csv('train_targets_scored.csv', header='true', inferSchema= 'true')\n",
    "df = df_train.join(df_label, on=['sig_id'], how='left_outer')  # Jjoin them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop vechile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.cp_type == 'trt_cp')\n",
    "df = df.drop('cp_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"cp_dose\", outputCol=\"cp_dose_cat\")\n",
    "df1 = indexer.fit(df).transform(df)\n",
    "indexer = StringIndexer(inputCol=\"cp_time\", outputCol=\"cp_time_cat\")\n",
    "df1 = indexer.fit(df1).transform(df1)\n",
    "df1 = df1.drop('cp_dose')\n",
    "df1 = df1.drop('cp_time')\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"cp_time_cat\", \"cp_dose_cat\"],\n",
    "                        outputCols=[\"cp_time_onehot\", \"cp_dose_onehot\"])\n",
    "\n",
    "model = encoder.fit(df1)\n",
    "df1 = model.transform(df1)\n",
    "df1 = df1.withColumn(\"cp_time_cols\", vector_to_array(\"cp_time_onehot\")).select(df1.columns + [col(\"cp_time_cols\")[i] for i in range(2)])\n",
    "df1 = df1.withColumn(\"cp_dose_cols\", vector_to_array(\"cp_dose_onehot\")).select(df1.columns + [col(\"cp_dose_cols\")[i] for i in range(1)])\n",
    "df1 = df1.drop('cp_dose_cat',\n",
    " 'cp_time_cat',\n",
    " 'cp_time_onehot',\n",
    " 'cp_dose_onehot',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering  (credit to Jeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_feature_names = [name for name in df1.columns if 'g-' in name]\n",
    "cell_feature_names =  [name for name in df1.columns if 'c-' in name]\n",
    "\n",
    "df2 = df1.withColumn(\"gene_max\", F.greatest(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"gene_min\", F.least(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_max\", F.greatest(*cell_feature_names))\n",
    "df2 = df2.withColumn(\"cell_min\", F.least(*cell_feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featrue engineering -- gene sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_sub_df = df2.select(*gene_feature_names)\n",
    "# gene_sub_df_T = spark.createDataFrame(gene_sub_df.toPandas().T)\n",
    "\n",
    "# w = Window.rowsBetween(-10,0)\n",
    "\n",
    "\n",
    "# gene_sub_df_T_roll = gene_sub_df_T.select(\n",
    "#     '*', \n",
    "#     *( F.avg(i).over(w).alias(i + '_roll') for i in gene_sub_df_T.columns)\n",
    "# ).drop(*gene_sub_df_T.columns)  # apply rolling average transformation for each sample\n",
    "# gene_rolled = spark.createDataFrame(gene_sub_df_T_roll.toPandas().T)\n",
    "\n",
    "# gene_rolled = gene_rolled.select([col(c).alias('g_' + c + '_rolled') for c in gene_rolled.columns])\\\n",
    "#                                 .drop(*gene_rolled.columns)  # just renameing the columns... that's all\n",
    "\n",
    "\n",
    "# window = Window.orderBy(F.col('monotonically_increasing_id'))\n",
    "# gene_rolled = gene_rolled.withColumn(\"monotonically_increasing_id\", F.monotonically_increasing_id())\\\n",
    "#                         .withColumn('row_number2', F.row_number().over(window))\\\n",
    "#                         .drop('monotonically_increasing_id')\n",
    "\n",
    "# gene_rolled.write.parquet('gene_rolled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_rolled = spark.read.parquet(\"gene_rolled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = Window.orderBy(\"monotonically_increasing_id\")\n",
    "df3 = df2.withColumn(\"monotonically_increasing_id\", F.monotonically_increasing_id())\\\n",
    "                        .withColumn('row_number2', F.row_number().over(window))\\\n",
    "                        .drop('monotonically_increasing_id')        \n",
    "\n",
    "df4 = df3.join(gene_rolled, on = 'row_number2', how = 'left')\\\n",
    "                .drop('row_number2')\\\n",
    "                .drop(*gene_feature_names)  # merging with the main frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop high correlation features -- mostly in cell features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = cell_feature_names  # This came from the previous section\n",
    "vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'feats' )\n",
    "feature_vector = vectorAssembler.transform(df2).select(\"feats\")\n",
    "# pyspark implementation of determining the correlations\n",
    "corr_matrix = Correlation.corr(feature_vector, \"feats\").head()[0]\n",
    "\n",
    "# Convert the correlation desne matrix and apply mask and to get the indicies where high correlations are observed\n",
    "# In here, I convert the correlation matrix to numpy, and then use numpy's mask to obtain the lower traingle of the\n",
    "# matrix. I used numpy becasue pyspark does not have mask.\n",
    "\n",
    "corr_Array = corr_matrix.toArray()\n",
    "masked_corr = np.ma.masked_where(np.triu(np.ones_like(corr_Array, dtype=bool)), corr_Array, copy=True)  \n",
    "idx_high_corr_feats = set(np.argwhere(masked_corr > 0.90)[:,0])  # Set threshold to 90%\n",
    "\n",
    "# Identify the column to drop and then drop it.\n",
    "features_to_drop = np.array(feature_columns)[list(idx_high_corr_feats)].tolist()\n",
    "\n",
    "\n",
    "# Finally\n",
    "\n",
    "df5 = df4.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_feature_names = list(set(df5.columns) - set(df_label.columns))\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = final_feature_names, outputCol = 'feats' )\n",
    "df6 = vectorAssembler.transform(df5).drop(*final_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100 \n",
    "pca = PCA(k = k, inputCol='feats', outputCol='pca_features')\n",
    "df7 = pca.fit(df6).transform(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_label(df, label_name):\n",
    "    \n",
    "    \n",
    "    if df.filter(df[label_name] == 1).count() >= 1:\n",
    "    \n",
    "        temp_df = df.select('pca_features', label_name)\n",
    "\n",
    "        # stratify split of the dataframe for train-test split\n",
    "        seed = 42\n",
    "        fractions = {1: 0.8, 0: 0.8}\n",
    "        train_df = temp_df.stat.sampleBy(label_name, fractions, seed, )\n",
    "        test_df =  temp_df.subtract(train_df)\n",
    "\n",
    "\n",
    "        # Over/down sampling of the training dataframe, becasue of the imbalanced class label\n",
    "\n",
    "        activation_samples = train_df.filter(train_df[label_name] == 1)\n",
    "        non_activation_samples = train_df.filter(train_df[label_name] == 0)\n",
    "        ratio = activation_samples.count() / non_activation_samples.count()\n",
    "\n",
    "        default_down_sample_ratio = 0.5  # This can be changed, but for now,i am just setting it as 0.5\n",
    "\n",
    "        upsample_ratio = default_down_sample_ratio / ratio\n",
    "\n",
    "        activation_samples_up = activation_samples.sample(True, upsample_ratio, 42)   # Upsample the activation samples\n",
    "\n",
    "        non_activation_samples_down = non_activation_samples.sample(True, default_down_sample_ratio, 42)  # Meanwhile, downsampling the non-activated samples \n",
    "\n",
    "\n",
    "        final_train_df = activation_samples_up.union(non_activation_samples_down).orderBy(F.rand())\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Finally, use CV to train the model and get the best parameters\n",
    "\n",
    "        clf = GBTClassifier(featuresCol='pca_features',  labelCol = label_name, maxIter=10)\n",
    "        paramGrid = ParamGridBuilder()\\\n",
    "                        .addGrid(clf.stepSize, [0.1,  0.05])\\\n",
    "                        .build()\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=label_name, metricName='logLoss')\n",
    "        crossval = CrossValidator(estimator=clf,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=2)  \n",
    "\n",
    "        cvModel = crossval.fit(final_train_df)\n",
    "        prediction_df = cvModel.transform(final_train_df)\n",
    "\n",
    "        cvModel.save(f\"./GBT/{label_name}.model\")\n",
    "        prediction_df.write.save(f\"./GBT/{label_name}_prediction_df.parquet\", format=\"parquet\")\n",
    "        final_train_df.write.save(f\"./GBT/{label_name}_train_df.parquet\", format=\"parquet\")\n",
    "\n",
    "        \n",
    "        prediction_test = cvModel.transform(test_df)\n",
    "        log_loss = evaluator.evaluate(prediction_test)\n",
    "\n",
    "        with open(f\"./GBT/log.log\", 'a') as f:\n",
    "            f.write(str(log_loss) + '\\n') \n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratify \n",
    "ref: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark/47672336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"./GBT/log.log\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "train_individual_label(df7,  '5-alpha_reductase_inhibitor',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 67/206 [9:56:46<22:07:23, 572.98s/it]"
     ]
    }
   ],
   "source": [
    "with open(f\"./logistics/log.log\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "\n",
    "temp_dict = {}\n",
    "for name in tqdm(df_label.columns[1:]):\n",
    "    temp_dict[name] = train_individual_label(df7,  name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_rolled.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sub_df = df2.select(*gene_feature_names)\n",
    "gene_sub_df_T = spark.createDataFrame(gene_sub_df.toPandas().T)\n",
    "\n",
    "# w = Window.rowsBetween(-10,0)\n",
    "\n",
    "\n",
    "# gene_sub_df_T_roll = gene_sub_df_T.select(\n",
    "#     '*', \n",
    "#     *( F.avg(i).over(w).alias(i + '_roll') for i in gene_sub_df_T.columns)\n",
    "# ).drop(*gene_sub_df_T.columns)  # apply rolling average transformation for each sample\n",
    "# gene_rolled = spark.createDataFrame(gene_sub_df_T_roll.toPandas().T)\n",
    "\n",
    "# gene_rolled.write.save(f\"gene_rolled_1.parquet\", format=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0b9570ec5e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgene_sub_df_T_roll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gene_sub_df_T_roll.toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sub_df_T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sub_df_T.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21938</th>\n",
       "      <th>21939</th>\n",
       "      <th>21940</th>\n",
       "      <th>21941</th>\n",
       "      <th>21942</th>\n",
       "      <th>21943</th>\n",
       "      <th>21944</th>\n",
       "      <th>21945</th>\n",
       "      <th>21946</th>\n",
       "      <th>21947</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>g-0</th>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.6111</td>\n",
       "      <td>2.044</td>\n",
       "      <td>0.2711</td>\n",
       "      <td>-0.3014</td>\n",
       "      <td>-0.0630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4123</td>\n",
       "      <td>-1.0140</td>\n",
       "      <td>1.7380</td>\n",
       "      <td>-0.1150</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>-0.8598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-1</th>\n",
       "      <td>0.5577</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>1.700</td>\n",
       "      <td>0.5133</td>\n",
       "      <td>0.5545</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1551</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>-1.2900</td>\n",
       "      <td>-0.8037</td>\n",
       "      <td>-0.3696</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>1.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-2</th>\n",
       "      <td>-0.2479</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>-0.9901</td>\n",
       "      <td>-1.539</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>-0.2576</td>\n",
       "      <td>-0.5279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8100</td>\n",
       "      <td>-0.4291</td>\n",
       "      <td>-0.4533</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>-0.1361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-3</th>\n",
       "      <td>-0.6208</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>5.944</td>\n",
       "      <td>2.5950</td>\n",
       "      <td>-0.1390</td>\n",
       "      <td>-0.2541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5042</td>\n",
       "      <td>1.8750</td>\n",
       "      <td>-1.1640</td>\n",
       "      <td>-0.1301</td>\n",
       "      <td>-0.2495</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.7952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-4</th>\n",
       "      <td>-0.1944</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>1.2810</td>\n",
       "      <td>-2.167</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>-0.6487</td>\n",
       "      <td>-0.0182</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.2380</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>-0.4841</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>-0.0175</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.3611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-767</th>\n",
       "      <td>-0.5582</td>\n",
       "      <td>-0.1214</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.5699</td>\n",
       "      <td>1.7660</td>\n",
       "      <td>0.1498</td>\n",
       "      <td>-4.810</td>\n",
       "      <td>-1.8910</td>\n",
       "      <td>1.5170</td>\n",
       "      <td>-0.0956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6917</td>\n",
       "      <td>0.2422</td>\n",
       "      <td>-0.4985</td>\n",
       "      <td>-0.5512</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.5243</td>\n",
       "      <td>-0.2297</td>\n",
       "      <td>-0.1022</td>\n",
       "      <td>-2.8720</td>\n",
       "      <td>-0.4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-768</th>\n",
       "      <td>0.3008</td>\n",
       "      <td>-0.1626</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>-1.0020</td>\n",
       "      <td>-0.4674</td>\n",
       "      <td>4.713</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>-0.4281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2549</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>0.1937</td>\n",
       "      <td>-0.4246</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.7221</td>\n",
       "      <td>0.5247</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>1.5410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-769</th>\n",
       "      <td>1.6490</td>\n",
       "      <td>-0.3340</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>0.4374</td>\n",
       "      <td>-0.7534</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>-5.431</td>\n",
       "      <td>0.6872</td>\n",
       "      <td>0.7831</td>\n",
       "      <td>0.6985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>1.2840</td>\n",
       "      <td>-0.0680</td>\n",
       "      <td>-0.3399</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.5099</td>\n",
       "      <td>0.5438</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-770</th>\n",
       "      <td>0.2968</td>\n",
       "      <td>-0.3289</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>4.011</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>-0.4754</td>\n",
       "      <td>-0.8000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1167</td>\n",
       "      <td>-0.8051</td>\n",
       "      <td>-0.4224</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.8418</td>\n",
       "      <td>-0.1423</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.3491</td>\n",
       "      <td>-3.1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g-771</th>\n",
       "      <td>-0.0224</td>\n",
       "      <td>-0.2718</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>-0.6269</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>2.534</td>\n",
       "      <td>1.3280</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.7218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1954</td>\n",
       "      <td>-0.2786</td>\n",
       "      <td>0.6780</td>\n",
       "      <td>-0.4424</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>-0.5982</td>\n",
       "      <td>0.3806</td>\n",
       "      <td>-0.4751</td>\n",
       "      <td>-0.4741</td>\n",
       "      <td>2.2190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772 rows × 21948 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3       4       5      6       7       8      \\\n",
       "g-0    1.0620  0.0743  0.6280 -0.5138 -0.3254 -0.6111  2.044  0.2711 -0.3014   \n",
       "g-1    0.5577  0.4087  0.5817 -0.2491 -0.4009  0.2941  1.700  0.5133  0.5545   \n",
       "g-2   -0.2479  0.2991  1.5540 -0.2656  0.9700 -0.9901 -1.539 -0.1327 -0.2576   \n",
       "g-3   -0.6208  0.0604 -0.0764  0.5288  0.6919  0.2277  5.944  2.5950 -0.1390   \n",
       "g-4   -0.1944  1.0190 -0.0323  4.0620  1.4180  1.2810 -2.167  0.6980 -0.6487   \n",
       "...       ...     ...     ...     ...     ...     ...    ...     ...     ...   \n",
       "g-767 -0.5582 -0.1214  0.8427  0.5699  1.7660  0.1498 -4.810 -1.8910  1.5170   \n",
       "g-768  0.3008 -0.1626  0.5797  0.1996 -1.0020 -0.4674  4.713  1.1250  0.1690   \n",
       "g-769  1.6490 -0.3340  0.3143  0.4374 -0.7534  0.9579 -5.431  0.6872  0.7831   \n",
       "g-770  0.2968 -0.3289  0.8133  0.1588  0.5000  0.1993  4.011  0.0641 -0.4754   \n",
       "g-771 -0.0224 -0.2718  0.7923 -0.0343 -0.6269  0.0592  2.534  1.3280  0.0371   \n",
       "\n",
       "        9      ...   21938   21939   21940   21941   21942   21943   21944  \\\n",
       "g-0   -0.0630  ...  0.4123 -1.0140  1.7380 -0.1150  0.1420  0.1608  0.1394   \n",
       "g-1    0.2564  ... -0.1551  0.1709 -1.2900 -0.8037 -0.3696 -1.0500 -0.0636   \n",
       "g-2   -0.5279  ...  1.8100 -0.4291 -0.4533  0.0988 -0.0093  0.2551 -0.1112   \n",
       "g-3   -0.2541  ...  0.5042  1.8750 -1.1640 -0.1301 -0.2495 -0.2239 -0.5080   \n",
       "g-4   -0.0182  ... -1.2380  0.9859 -0.4841  0.2013 -0.0175 -0.2431 -0.4713   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "g-767 -0.0956  ... -0.6917  0.2422 -0.4985 -0.5512  0.0186  0.5243 -0.2297   \n",
       "g-768 -0.4281  ... -0.2549  0.0374  0.9495  0.1937 -0.4246 -0.0003  0.7221   \n",
       "g-769  0.6985  ...  0.3235  1.2840 -0.0680 -0.3399  0.0017  0.1715  0.5099   \n",
       "g-770 -0.8000  ... -0.1167 -0.8051 -0.4224  0.2947 -0.4474  0.8418 -0.1423   \n",
       "g-771  0.7218  ... -0.1954 -0.2786  0.6780 -0.4424  0.1966 -0.5982  0.3806   \n",
       "\n",
       "        21945   21946   21947  \n",
       "g-0   -1.3260  0.6660 -0.8598  \n",
       "g-1    0.3478  0.2324  1.0240  \n",
       "g-2   -0.3743  0.4392 -0.1361  \n",
       "g-3    0.9905  0.2044  0.7952  \n",
       "g-4   -0.7178  0.8531 -0.3611  \n",
       "...       ...     ...     ...  \n",
       "g-767 -0.1022 -2.8720 -0.4017  \n",
       "g-768  0.5247  0.1794  1.5410  \n",
       "g-769  0.5438  0.3109  0.3633  \n",
       "g-770 -0.1875 -0.3491 -3.1970  \n",
       "g-771 -0.4751 -0.4741  2.2190  \n",
       "\n",
       "[772 rows x 21948 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_sub_df.toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"Product,Category,Revenue\n",
    "Thin,phone,6000\n",
    "Normal,tablet,1500\n",
    "Mini,tablet,5500\n",
    "Ultra Thin,phone,5000\n",
    "Very Thin,phone,6000\n",
    "Big,tablet,2500\n",
    "Bendable,phone,3000\n",
    "Foldable,phone,3000\n",
    "Pro,tablet,4500\n",
    "Pro+,tablet,6500\"\"\"\n",
    "\n",
    "with open(\"example.csv\", \"w\") as f:\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+--------------------------------------------------------------+\n",
      "|   Product|Category|Revenue|avg(Revenue) OVER (ROWS BETWEEN -10 FOLLOWING AND CURRENT ROW)|\n",
      "+----------+--------+-------+--------------------------------------------------------------+\n",
      "|      Thin|   phone|   6000|                                                        6000.0|\n",
      "|    Normal|  tablet|   1500|                                                        3750.0|\n",
      "|      Mini|  tablet|   5500|                                             4333.333333333333|\n",
      "|Ultra Thin|   phone|   5000|                                                        4500.0|\n",
      "| Very Thin|   phone|   6000|                                                        4800.0|\n",
      "|       Big|  tablet|   2500|                                             4416.666666666667|\n",
      "|  Bendable|   phone|   3000|                                             4214.285714285715|\n",
      "|  Foldable|   phone|   3000|                                                        4062.5|\n",
      "|       Pro|  tablet|   4500|                                             4111.111111111111|\n",
      "|      Pro+|  tablet|   6500|                                                        4350.0|\n",
      "+----------+--------+-------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.rowsBetween(-10,0)\n",
    "\n",
    "dfdddd = spark.read.csv('example.csv', header='true',inferSchema=True)   # path in HDFS file system\n",
    "\n",
    "\n",
    "dfdddd.select(\n",
    "    '*', \n",
    "    F.avg('Revenue').over(w)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+\n",
      "|   Product|Category|Revenue|\n",
      "+----------+--------+-------+\n",
      "|      Thin|   phone|   6000|\n",
      "|    Normal|  tablet|   1500|\n",
      "|      Mini|  tablet|   5500|\n",
      "|Ultra Thin|   phone|   5000|\n",
      "| Very Thin|   phone|   6000|\n",
      "|       Big|  tablet|   2500|\n",
      "|  Bendable|   phone|   3000|\n",
      "|  Foldable|   phone|   3000|\n",
      "|       Pro|  tablet|   4500|\n",
      "|      Pro+|  tablet|   6500|\n",
      "+----------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfdddd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5511764976d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Geeks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geeks'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "my_set = {'Geeks', 'for', 'geeks'} \n",
    "  \n",
    "s = list(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cp_type|\n",
      "+-------+\n",
      "| trt_cp|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cp_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'0.8'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "list = [(2147481832,23355149,1),(2147481832,973010692,1),(2147481832,2134870842,1),(2147481832,541023347,1),(2147481832,1682206630,1),(2147481832,1138211459,1),(2147481832,852202566,1),(2147481832,201375938,1),(2147481832,486538879,1),(2147481832,919187908,1),(214748183,919187908,1),(214748183,91187908,1)]\n",
    "df = spark.createDataFrame(list, [\"x1\",\"x2\",\"x3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: bigint, x2: bigint, x3: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2147481832: 0.8, 214748183: 0.8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|        x1|fraction|\n",
      "+----------+--------+\n",
      "|2147481832|     0.8|\n",
      "| 214748183|     0.8|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = features_and_targets.withColumn('target_vector', (vector_to_string(array([features_and_targets[col] for col in target_names])))).select(['sig_id', 'target_vector'])\n",
    "string_indexer = StringIndexer(inputCol = 'target_vector', outputCol = 'target')\n",
    "string_indexer_model = string_indexer.fit(temp_df)\n",
    "temp_df = string_indexer_model.transform(temp_df).drop('target_vector')\n",
    "\n",
    "data = features_and_targets.join(temp_df, features_and_targets.sig_id == temp_df.sig_id, how = 'inner').drop(temp_df.sig_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
