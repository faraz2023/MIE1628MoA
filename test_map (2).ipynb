{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b4d6e2a-136d-4d16-84be-e03ea720e595",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b7f0911-85b5-46b1-b3ac-eb24faf51658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-336631661066794&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> config<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.dynamicAllocation.maxExecutors&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;8&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> config<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.dynamicAllocation.initialExecutors&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;6&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">;</span> <span class=\"ansi-red-fg\"># the number must be between the min and max</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">(</span>conf<span class=\"ansi-blue-fg\">=</span>config<span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># start a new sc with the current config</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> spark <span class=\"ansi-blue-fg\">=</span> SparkSession<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 &#34; is not allowed as it is a security risk.&#34;)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    133</span> \n",
       "<span class=\"ansi-green-fg\">--&gt; 134</span><span class=\"ansi-red-fg\">         </span>SparkContext<span class=\"ansi-blue-fg\">.</span>_ensure_initialized<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> gateway<span class=\"ansi-blue-fg\">=</span>gateway<span class=\"ansi-blue-fg\">,</span> conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    136</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">_ensure_initialized</span><span class=\"ansi-blue-fg\">(cls, instance, gateway, conf)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    345</span>                         <span class=\"ansi-blue-fg\">&#34; created by %s at %s:%s &#34;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    346</span>                         % (currentAppName, currentMaster,\n",
       "<span class=\"ansi-green-fg\">--&gt; 347</span><span class=\"ansi-red-fg\">                             callsite.function, callsite.file, callsite.linenum))\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    348</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    349</span>                     SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context <span class=\"ansi-blue-fg\">=</span> instance\n",
       "\n",
       "<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /local_disk0/tmp/1606610146529-0/PythonShell.py:1686 </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-336631661066794&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> config<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.dynamicAllocation.maxExecutors&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;8&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> config<span class=\"ansi-blue-fg\">.</span>set<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.dynamicAllocation.initialExecutors&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;6&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">;</span> <span class=\"ansi-red-fg\"># the number must be between the min and max</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\"> </span>sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">(</span>conf<span class=\"ansi-blue-fg\">=</span>config<span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># start a new sc with the current config</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> spark <span class=\"ansi-blue-fg\">=</span> SparkSession<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    132</span>                 &#34; is not allowed as it is a security risk.&#34;)\n<span class=\"ansi-green-intense-fg ansi-bold\">    133</span> \n<span class=\"ansi-green-fg\">--&gt; 134</span><span class=\"ansi-red-fg\">         </span>SparkContext<span class=\"ansi-blue-fg\">.</span>_ensure_initialized<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> gateway<span class=\"ansi-blue-fg\">=</span>gateway<span class=\"ansi-blue-fg\">,</span> conf<span class=\"ansi-blue-fg\">=</span>conf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    135</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    136</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">_ensure_initialized</span><span class=\"ansi-blue-fg\">(cls, instance, gateway, conf)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    345</span>                         <span class=\"ansi-blue-fg\">&#34; created by %s at %s:%s &#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    346</span>                         % (currentAppName, currentMaster,\n<span class=\"ansi-green-fg\">--&gt; 347</span><span class=\"ansi-red-fg\">                             callsite.function, callsite.file, callsite.linenum))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    348</span>                 <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    349</span>                     SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context <span class=\"ansi-blue-fg\">=</span> instance\n\n<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /local_disk0/tmp/1606610146529-0/PythonShell.py:1686 </div>",
       "errorSummary": "<span class=\"ansi-red-fg\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /local_disk0/tmp/1606610146529-0/PythonShell.py:1686 ",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '10'), ('spark.cores.max', '12'), ('spark.driver.memory','8g')])\n",
    "config.setAppName(\"proj\")\n",
    "config.set(\"spark.dynamicAllocation.minExecutors\", \"4\");\n",
    "config.set(\"spark.dynamicAllocation.maxExecutors\", \"8\");\n",
    "config.set(\"spark.dynamicAllocation.initialExecutors\", \"6\"); # the number must be between the min and max\n",
    "sc = SparkContext(conf=config)  # start a new sc with the current config\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d7ed8f0-5b21-45df-88b4-2e76689a3f81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7d026dda-8a12-4748-998f-074956a532a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [
        {
         "name": "feature_df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "sig_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "5-alpha_reductase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "11-beta-hsd1_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "acat_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "acetylcholine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "acetylcholine_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "acetylcholinesterase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "adenosine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "adenosine_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "adenylyl_cyclase_activator",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "adrenergic_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "adrenergic_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "akt_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "aldehyde_dehydrogenase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "alk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ampk_activator",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "analgesic",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "androgen_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "androgen_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "anesthetic_-_local",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "angiogenesis_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "angiotensin_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "anti-inflammatory",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antiarrhythmic",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antibiotic",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "anticonvulsant",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antifungal",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antihistamine",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antimalarial",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antioxidant",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antiprotozoal",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "antiviral",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "apoptosis_stimulant",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "aromatase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "atm_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "atp-sensitive_potassium_channel_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "atp_synthase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "atpase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "atr_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "aurora_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "autotaxin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_30s_ribosomal_subunit_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_50s_ribosomal_subunit_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_antifolate",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_cell_wall_synthesis_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_dna_gyrase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_dna_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bacterial_membrane_integrity_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bcl_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bcr-abl_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "benzodiazepine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "beta_amyloid_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "bromodomain_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "btk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "calcineurin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "calcium_channel_blocker",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cannabinoid_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cannabinoid_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "carbonic_anhydrase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "casein_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "caspase_activator",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "catechol_o_methyltransferase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cc_chemokine_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cck_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cdk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "chelating_agent",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "chk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "chloride_channel_blocker",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cholesterol_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cholinergic_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "coagulation_factor_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "corticosteroid_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cyclooxygenase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "cytochrome_p450_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dihydrofolate_reductase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dipeptidyl_peptidase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "diuretic",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dna_alkylating_agent",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dna_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dopamine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "dopamine_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "egfr_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "elastase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "erbb2_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "estrogen_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "estrogen_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "faah_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "farnesyltransferase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "fatty_acid_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "fgfr_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "flt3_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "focal_adhesion_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "free_radical_scavenger",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "fungal_squalene_epoxidase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "gaba_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "gaba_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "gamma_secretase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "glucocorticoid_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "glutamate_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "glutamate_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "glutamate_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "gonadotropin_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "gsk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "hcv_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "hdac_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "histamine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "histamine_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "histone_lysine_demethylase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "histone_lysine_methyltransferase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "hiv_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "hmgcr_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "hsp_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "igf-1_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ikk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "imidazoline_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "immunosuppressant",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "insulin_secretagogue",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "insulin_sensitizer",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "integrin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "jak_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "kit_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "laxative",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "leukotriene_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "leukotriene_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "lipase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "lipoxygenase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "lxr_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "mdm_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "mek_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "membrane_integrity_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "mineralocorticoid_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "monoacylglycerol_lipase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "monoamine_oxidase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "monopolar_spindle_1_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "mtor_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "mucolytic_agent",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "neuropeptide_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nfkb_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nicotinic_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nitric_oxide_donor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nitric_oxide_production_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nitric_oxide_synthase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "norepinephrine_reuptake_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "nrf2_activator",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "opioid_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "opioid_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "orexin_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "p38_mapk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "p-glycoprotein_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "parp_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "pdgfr_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "pdk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "phosphodiesterase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "phospholipase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "pi3k_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "pkc_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "potassium_channel_activator",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "potassium_channel_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ppar_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ppar_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "progesterone_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "progesterone_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "prostaglandin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "prostanoid_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "proteasome_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "protein_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "protein_phosphatase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "protein_synthesis_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "protein_tyrosine_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "radiopaque_medium",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "raf_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ras_gtpase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "retinoid_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "retinoid_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "rho_associated_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ribonucleoside_reductase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "rna_polymerase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "serotonin_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "serotonin_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "serotonin_reuptake_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sigma_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sigma_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "smoothened_receptor_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sodium_channel_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "sphingosine_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "src_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "steroid",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "syk_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tachykinin_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tgf-beta_receptor_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "thrombin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "thymidylate_synthase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tlr_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tlr_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tnf_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "topoisomerase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "transient_receptor_potential_channel_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tropomyosin_receptor_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "trpv_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "trpv_antagonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tubulin_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "tyrosine_kinase_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "ubiquitin_specific_protease_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "vegfr_inhibitor",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "vitamin_b",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "vitamin_d_receptor_agonist",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "wnt_inhibitor",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_df = spark.read.csv('/FileStore/tables/train_targets_scored.csv', header='true', inferSchema= 'true')   # path in HDFS file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below: transpose dataframe such that it will have the size of num_of_labels x num_of_samples..  \n",
    "Remember the original target dataframe has the size of num_of_samples x num_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60926af1-b727-4236-9989-aff1a48f1cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_df_transpose = spark.createDataFrame(feature_df.select(*feature_df.columns[1:]).toPandas().T)  # Tranposed label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d8ad1363-7353-47f6-9abe-ab7997b11957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_func1(x):\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3e7896ce-6a98-495e-a30b-71ec4fe8cc12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = vectorized_df_transpose.rdd.map(test_func1).collect() # This runs fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9bd3cef-7bc0-4739-b71b-5314f916584f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[8]: [None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[8]: [None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None,\n None]</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "48b2f3ac-a970-483f-859c-ebfe55675108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def test_func2(x):\n",
    "    rf = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "#     an_arbitary_pyspark_dataframe_object = spark.createDataFrame([(1,2), (3,4)])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ecae947d-d101-4049-9bfb-013511dfc9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-336631661066800&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>_ <span class=\"ansi-blue-fg\">=</span> vectorized_df_transpose<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>test_func2<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n",
       "<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 42, ip-10-172-247-11.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?&#39;, from &lt;command-336631661066798&gt;, line 4. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n",
       "    process()\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File &#34;&lt;command-336631661066798&gt;&#34;, line 4, in test_func2\n",
       "  File &#34;/databricks/spark/python/pyspark/__init__.py&#34;, line 110, in wrapper\n",
       "    return func(self, **kwargs)\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/classification.py&#34;, line 655, in __init__\n",
       "    &#34;org.apache.spark.ml.classification.LogisticRegression&#34;, self.uid)\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/wrapper.py&#34;, line 65, in _new_java_obj\n",
       "    java_obj = _jvm()\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/util.py&#34;, line 85, in _jvm\n",
       "    raise AttributeError(&#34;Cannot load _jvm from SparkContext. Is SparkContext initialized?&#34;)\n",
       "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n",
       "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n",
       "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.api.python.PythonException: &#39;AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?&#39;, from &lt;command-336631661066798&gt;, line 4. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n",
       "    process()\n",
       "  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n",
       "    serializer.dump_stream(out_iter, outfile)\n",
       "  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n",
       "    vs = list(itertools.islice(iterator, batch))\n",
       "  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n",
       "    return f(*args, **kwargs)\n",
       "  File &#34;&lt;command-336631661066798&gt;&#34;, line 4, in test_func2\n",
       "  File &#34;/databricks/spark/python/pyspark/__init__.py&#34;, line 110, in wrapper\n",
       "    return func(self, **kwargs)\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/classification.py&#34;, line 655, in __init__\n",
       "    &#34;org.apache.spark.ml.classification.LogisticRegression&#34;, self.uid)\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/wrapper.py&#34;, line 65, in _new_java_obj\n",
       "    java_obj = _jvm()\n",
       "  File &#34;/databricks/spark/python/pyspark/ml/util.py&#34;, line 85, in _jvm\n",
       "    raise AttributeError(&#34;Cannot load _jvm from SparkContext. Is SparkContext initialized?&#34;)\n",
       "AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
       "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
       "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
       "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
       "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
       "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
       "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
       "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
       "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
       "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n",
       "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-336631661066800&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>_ <span class=\"ansi-blue-fg\">=</span> vectorized_df_transpose<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>test_func2<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 42, ip-10-172-247-11.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?&#39;, from &lt;command-336631661066798&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-336631661066798&gt;&#34;, line 4, in test_func2\n  File &#34;/databricks/spark/python/pyspark/__init__.py&#34;, line 110, in wrapper\n    return func(self, **kwargs)\n  File &#34;/databricks/spark/python/pyspark/ml/classification.py&#34;, line 655, in __init__\n    &#34;org.apache.spark.ml.classification.LogisticRegression&#34;, self.uid)\n  File &#34;/databricks/spark/python/pyspark/ml/wrapper.py&#34;, line 65, in _new_java_obj\n    java_obj = _jvm()\n  File &#34;/databricks/spark/python/pyspark/ml/util.py&#34;, line 85, in _jvm\n    raise AttributeError(&#34;Cannot load _jvm from SparkContext. Is SparkContext initialized?&#34;)\nAttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?&#39;, from &lt;command-336631661066798&gt;, line 4. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-336631661066798&gt;&#34;, line 4, in test_func2\n  File &#34;/databricks/spark/python/pyspark/__init__.py&#34;, line 110, in wrapper\n    return func(self, **kwargs)\n  File &#34;/databricks/spark/python/pyspark/ml/classification.py&#34;, line 655, in __init__\n    &#34;org.apache.spark.ml.classification.LogisticRegression&#34;, self.uid)\n  File &#34;/databricks/spark/python/pyspark/ml/wrapper.py&#34;, line 65, in _new_java_obj\n    java_obj = _jvm()\n  File &#34;/databricks/spark/python/pyspark/ml/util.py&#34;, line 85, in _jvm\n    raise AttributeError(&#34;Cannot load _jvm from SparkContext. Is SparkContext initialized?&#34;)\nAttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 17.0 failed 1 times, most recent failure: Lost task 0.0 in stage 17.0 (TID 42, ip-10-172-247-11.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;AttributeError: Cannot load _jvm from SparkContext. Is SparkContext initialized?&#39;, from &lt;command-336631661066798&gt;, line 4. Full traceback below:",
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = vectorized_df_transpose.rdd.map(test_func2).collect()  # This will not run propoerly, meaning that you are unable to run ML algorithm inside map function in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be29a4c3-41d0-44e2-b255-748a4efbf33f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "See also [https://stackoverflow.com/questions/43428297/run-ml-algorithm-inside-map-function-in-spark](https://stackoverflow.com/questions/43428297/run-ml-algorithm-inside-map-function-in-spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "test_map",
   "notebookOrigID": 336631661066789,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
