{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanisms of Action Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Members: Abhi Gandhi, Jeff Won, Shashank Saurav, Shengebo Zhang, Faraz Khoshbakhtian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document outlines the steps taken by our group to approach the Mechanisms of Action competition. This document presents are original approach whereas another document presents the challanger approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "### This part is to run pyspark locally\n",
    "import findspark  # Get rid of this in DataBricks\n",
    "findspark.init('/opt/spark-3.0.1')  # Get rid of this in DataBricks #faraz: you can remove the parameter. it only worked like this for me\n",
    "########################################\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10, sqrt\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeting up spark, contexts and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/jwillow/anaconda3/lib/python3.7/site-packages/IPython/utils/py3compat.py:168 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f38f8fff9b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.dynamicAllocation.maxExecutors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.dynamicAllocation.initialExecutors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m# the number must be between the min and max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# start a new sc with the current config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 336\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/jwillow/anaconda3/lib/python3.7/site-packages/IPython/utils/py3compat.py:168 "
     ]
    }
   ],
   "source": [
    "config = SparkConf().setAll([('spark.executor.memory', '30g'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','8g')])\n",
    "config.setAppName(\"proj\")\n",
    "config.set(\"spark.dynamicAllocation.minExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.maxExecutors\", \"2\");\n",
    "config.set(\"spark.dynamicAllocation.initialExecutors\", \"2\"); # the number must be between the min and max\n",
    "sc = SparkContext(conf=config)  # start a new sc with the current config\n",
    "spark = SparkSession(sc)\n",
    "sqlc=SQLContext(sc)\n",
    "print(sc.getConf().getAll())  # print all the configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing the Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Joining dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to read the features and labels for the training data, get rid of controll cases and join all the training data into a singular Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for being able to store the data in github and concatenation on local computer\n",
    "!cat train_features_*.csv > train_feats.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Features Dataset\n",
    "df_train = spark.read.csv('train_feats.csv', header='true', inferSchema= 'true')   # path in HDFS file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels Dataset\n",
    "df_label = spark.read.csv('train_targets_scored.csv', header='true', inferSchema= 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data together\n",
    "df = df_train.join(df_label, on=['sig_id'], how='left_outer')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the control cases since the test data also comes with control/case flag\n",
    "df = df.filter(df.cp_type == 'trt_cp')\n",
    "df = df.drop('cp_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding for Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to **One-Hot Encode** our categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexer for cp_dose\n",
    "indexer = StringIndexer(inputCol=\"cp_dose\", outputCol=\"cp_dose_cat\")\n",
    "# index cp_dose in data\n",
    "df1 = indexer.fit(df).transform(df)\n",
    "\n",
    "# String indexer for cp_time\n",
    "indexer = StringIndexer(inputCol=\"cp_time\", outputCol=\"cp_time_cat\")\n",
    "# index cp_time in data\n",
    "df1 = indexer.fit(df1).transform(df1)\n",
    "df1 = df1.drop('cp_dose')\n",
    "df1 = df1.drop('cp_time')\n",
    "\n",
    "# One-hot enocder \n",
    "encoder = OneHotEncoder(inputCols=[\"cp_time_cat\", \"cp_dose_cat\"],\n",
    "                        outputCols=[\"cp_time_onehot\", \"cp_dose_onehot\"])\n",
    "\n",
    "model = encoder.fit(df1)\n",
    "df1 = model.transform(df1)\n",
    "\n",
    "# Keeping n-1 dummy variables for each feature. (dummy variables have degree of fredom n-1)\n",
    "df1 = df1.withColumn(\"cp_time_cols\", vector_to_array(\"cp_time_onehot\")).select(df1.columns + [col(\"cp_time_cols\")[i] for i in range(2)])\n",
    "df1 = df1.withColumn(\"cp_dose_cols\", vector_to_array(\"cp_dose_onehot\")).select(df1.columns + [col(\"cp_dose_cols\")[i] for i in range(1)])\n",
    "\n",
    "# drop leftover cols\n",
    "df1 = df1.drop('cp_dose_cat',\n",
    " 'cp_time_cat',\n",
    " 'cp_time_onehot',\n",
    " 'cp_dose_onehot',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we create some features that describes the statistics of each row. We look at the row-wise min, max, mean, standard deviation for gene and cell information. We also use KMeans clustering to classify the data into 3 cluster classes based on Euclidean distance and use the cluster results as an additional engineered feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|clusterClassPrediction|\n",
      "+----------------------+\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     0|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     1|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gene_feature_names = [name for name in df1.columns if 'g-' in name]\n",
    "cell_feature_names =  [name for name in df1.columns if 'c-' in name]\n",
    "\n",
    "df2 = df1.withColumn(\"gene_max\", F.greatest(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"gene_min\", F.least(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_max\", F.greatest(*cell_feature_names))\n",
    "df2 = df2.withColumn(\"cell_min\", F.least(*cell_feature_names))\n",
    "\n",
    "\n",
    "df2 = df2.withColumn(\"gene_mean\", reduce(lambda x,y: x+y, (col(x) for x in gene_feature_names)) / len(gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_mean\", reduce(lambda x,y: x+y, (col(x) for x in cell_feature_names)) / len(cell_feature_names))\n",
    "\n",
    "\n",
    "gene_std = sqrt(\n",
    "    reduce(add, ((col(x) - col(\"gene_mean\")) ** 2 for x in gene_feature_names)) / (len(gene_feature_names) - 1)\n",
    ")\n",
    "\n",
    "cell_std = sqrt(\n",
    "    reduce(add, ((col(x) - col(\"cell_mean\")) ** 2 for x in cell_feature_names)) / (len(cell_feature_names) - 1)\n",
    ")\n",
    "\n",
    "df2 = df2.withColumn(\"gene_std\", gene_std)\n",
    "df2 = df2.withColumn(\"cell_std\", cell_std)\n",
    "# df2 = df2.withColumn(\"gene_std\", sqrt(reduce(lambda x,y: x-col('gene_mean') + y-col('gene_mean'), (col(x) for x in gene_feature_names))**2 / len(gene_feature_names)))\n",
    "# df2 = df2.withColumn(\"cell_std\", sqrt(reduce(lambda x,y: x-col('cell_mean') + y-col('cell_mean'), (col(x) for x in cell_feature_names))**2 / len(cell_feature_names)))\n",
    "\n",
    "\n",
    "df2 = df2.withColumn(\"gene_sum\", reduce(lambda x,y: x+y, (col(x) for x in gene_feature_names)))\n",
    "df2 = df2.withColumn(\"cell_sum\", reduce(lambda x,y: x+y, (col(x) for x in cell_feature_names)))\n",
    "\n",
    "\n",
    "'''\n",
    "KMeans clustering - engineer new feature based on cluster results\n",
    "Use K=3 to fit the assembled features\n",
    "'''\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "feature_label_assembler = VectorAssembler(inputCols=df1.columns[1:], outputCol=\"assemebled\")\n",
    "cluster_df = feature_label_assembler.transform(df1)\n",
    "\n",
    "kmeans = KMeans(k=3, featuresCol='assemebled', \n",
    "                predictionCol='clusterClassPrediction', distanceMeasure='euclidean',)\n",
    "\n",
    "model = kmeans.fit(cluster_df)\n",
    "transformed = model.transform(cluster_df).select(\"sig_id\", \"clusterClassPrediction\")\n",
    "\n",
    "df2 = df2.join(transformed, on=['sig_id'], how='inner')\n",
    "df2.select(\"clusterClassPrediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Purpose Feature Selection Based on Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial step for feature selection we use correlation meassure to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping track of the feature names\n",
    "feature_columns = gene_feature_names + cell_feature_names \n",
    "\n",
    "# Creating the feature vector\n",
    "vectorAssembler = VectorAssembler(inputCols = feature_columns, outputCol = 'feats' )\n",
    "feature_vector = vectorAssembler.transform(df2).select(\"feats\")\n",
    "\n",
    "# Computing the correlations with pySpark\n",
    "corr_matrix = Correlation.corr(feature_vector, \"feats\").head()[0]\n",
    "\n",
    "# Convert the correlation desne matrix and apply mask and to get the indicies where high correlations are observed\n",
    "# In here, I convert the correlation matrix to numpy, and then use numpy's mask to obtain the lower traingle of the\n",
    "# matrix. I used numpy becasue pyspark does not have mask.\n",
    "\n",
    "# Detection highly correlated features\n",
    "corr_Array = corr_matrix.toArray()\n",
    "masked_corr = np.ma.masked_where(np.triu(np.ones_like(corr_Array, dtype=bool)), corr_Array, copy=True)  \n",
    "idx_high_corr_feats = set(np.argwhere(abs(masked_corr) > 0.90)[:,0])  # Set threshold to 90%\n",
    "# Identify the column to drop \n",
    "features_to_drop = np.array(feature_columns)[list(idx_high_corr_feats)].tolist()\n",
    "\n",
    "\n",
    "# Drop the correalted features\n",
    "df3 = df2.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for Mechanisms of Action is formally considred a Multi-label classification problem. We are employing the Binary-Relevance approach for the problem at hand. For each of the possible binary labels we first use a **Random Forrest CLassifier** to achive another level of feature selection based on the feature importance meassure. Subsequently, we train one **logistic regression** model and a **naive bayes** model for each label.\n",
    "\n",
    "We have use upsampling the minority class and downsampling the majority class to overcome issues of imbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the final feature set\n",
    "final_feature_names = list(set(df3.columns) - set(df_label.columns))\n",
    "\n",
    "# Create Feature vector\n",
    "vectorAssembler = VectorAssembler(inputCols = final_feature_names, outputCol = 'feats' )\n",
    "\n",
    "# drop all unnecessary features\n",
    "df4 = vectorAssembler.transform(df3).drop(*final_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-afad608fa216>, line 83)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-afad608fa216>\"\u001b[0;36m, line \u001b[0;32m83\u001b[0m\n\u001b[0;31m    .build()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# This function is responsible for running our ml pipeline\n",
    "\n",
    "#df is the train dataset, label_name is the label we want to do the training on\n",
    "def train_individual_label(df, label_name, seed = 42):\n",
    "    \n",
    "    # we need at least 2 positive instances for this particular label_name\n",
    "    if df.filter(df[label_name] == 1).count() >= 2:\n",
    "    \n",
    "        # dataframe consisting only of the train features and the label name\n",
    "        temp_df = df.select('feats', label_name)\n",
    "\n",
    "        # stratify split of the dataframe for train-test split\n",
    "        fractions = {1: 0.8, 0: 0.8}\n",
    "        train_df = temp_df.stat.sampleBy(label_name, fractions, seed, )\n",
    "        test_df =  temp_df.subtract(train_df)\n",
    "\n",
    "\n",
    "        # Over/down sampling of the training dataframe, becasue of the imbalanced class label\n",
    "        activation_samples = train_df.filter(train_df[label_name] == 1)\n",
    "        non_activation_samples = train_df.filter(train_df[label_name] == 0)\n",
    "        ratio = activation_samples.count() / non_activation_samples.count()\n",
    "\n",
    "        # somewhat arbitrary ration meassure\n",
    "        default_down_sample_ratio = 0.5  \n",
    "        upsample_ratio = default_down_sample_ratio / ratio\n",
    "\n",
    "        activation_samples_up = activation_samples.sample(True, upsample_ratio, seed)   # Upsample the activation samples\n",
    "        non_activation_samples_down = non_activation_samples.sample(True, default_down_sample_ratio, seed)  # Meanwhile, downsampling the non-activated samples \n",
    "\n",
    "        # randomise the order of samples\n",
    "        final_train_df = activation_samples_up.union(non_activation_samples_down).orderBy(F.rand())\n",
    "\n",
    "\n",
    "        # another layer of feature selection using random forrest before training the ml models\n",
    "        clf = RandomForestClassifier(numTrees=20, maxDepth=5, labelCol = label_name, featuresCol='feats',  seed=42)\n",
    "        model = clf.fit(final_train_df)\n",
    "        feature_importance = model.featureImportances.toArray()\n",
    "        \n",
    "        # indeces of only the top 10% of features\n",
    "        important_feature_idx = feature_importance.argsort()[-int(0.1 * len(feature_importance)):]  \n",
    "                                                                                                   \n",
    "\n",
    "\n",
    "        # Now, after getting the index, filter the feature vector based on the above feature importance index\n",
    "        slicer = VectorSlicer(inputCol=\"feats\", outputCol=\"feats_sub\", indices=important_feature_idx)\n",
    "        final_train_df_sub_feats =  slicer.transform(final_train_df).drop('feats')\n",
    "        final_test_df_sub_feats = slicer.transform(test_df).drop('feats')\n",
    "\n",
    "\n",
    "\n",
    "        # Use CV to train a logistic regression model\n",
    "        lr = LogisticRegression(maxIter=10, featuresCol='feats_sub',  labelCol = label_name)\n",
    "        lr_paramGrid = ParamGridBuilder() \\\n",
    "                        .addGrid(lr.regParam, [ 0.1, 0.01]) \\\n",
    "                        .addGrid(lr.elasticNetParam, [1,  0])\\\n",
    "                        .build()\n",
    "\n",
    "        \n",
    "        lr_evaluator = MulticlassClassificationEvaluator(labelCol=label_name, metricName='logLoss')\n",
    "        lr_crossval = CrossValidator(estimator=lr,\n",
    "                              estimatorParamMaps=lr_paramGrid,\n",
    "                              evaluator=lr_evaluator,\n",
    "                              numFolds=3)  \n",
    "\n",
    "        lr_cvModel = lr_crossval.fit(final_train_df_sub_feats)\n",
    "        lr_prediction_df = lr_cvModel.transform(final_test_df_sub_feats)\n",
    "\n",
    "        lr_cvModel.save(f\"./logistics/{label_name}.model\")\n",
    "        lr_prediction_df.write.save(f\"./logistics/{label_name}_prediction_df.parquet\", format=\"parquet\")\n",
    "        final_train_df_sub_feats.write.save(f\"./logistics/{label_name}_train_df.parquet\", format=\"parquet\")\n",
    "\n",
    "        lr_log_loss = lr_evaluator.evaluate(test_df)\n",
    "\n",
    "        with open(f\"./logistics/log.log\", 'a') as f:\n",
    "            f.write(str(lr_log_loss) + '\\n') \n",
    "            \n",
    "        ##########\n",
    "        \n",
    "        # Use CV to train a naive-bayes classifer\n",
    "        nb = NaiveBayes(maxIter=10, featuresCol='feats_sub',  labelCol = label_name)\n",
    "        nb_paramGrid = ParamGridBuilder() \\\n",
    "                                        .addGrid(nb.smoothing, [0.8, 1.0])\n",
    "                                        .build()\n",
    "\n",
    "        nb_evaluator = MulticlassClassificationEvaluator(labelCol=label_name, metricName='logLoss')\n",
    "        nb_crossval = CrossValidator(estimator=nb,\n",
    "                              estimatorParamMaps=nb_paramGrid,\n",
    "                              evaluator=nb_evaluator,\n",
    "                              numFolds=3)  \n",
    "\n",
    "        nb_cvModel = nb_crossval.fit(final_train_df_sub_feats)\n",
    "        nb_prediction_df = nb_cvModel.transform(final_test_df_sub_feats)\n",
    "\n",
    "        nb_cvModel.save(f\"./naive-bayes/{label_name}.model\")\n",
    "\n",
    "        nb_log_loss = nb_evaluator.evaluate(test_df)\n",
    "\n",
    "        with open(f\"./naive-bayes/log.log\", 'a') as f:\n",
    "            f.write(str(nb_log_loss) + '\\n') \n",
    "\n",
    "        return (True, log_loss)\n",
    "    else:\n",
    "        return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_individual_label = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratify \n",
    "ref: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark/47672336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './naive-bayes/log.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-195867efbb8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./logistics/log.log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./naive-bayes/log.log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './naive-bayes/log.log'"
     ]
    }
   ],
   "source": [
    "with open(f\"./logistics/log.log\", 'w') as f:\n",
    "    pass\n",
    "with open(f\"./naive-bayes/log.log\", 'w') as f:\n",
    "    pass\n",
    "\n",
    "\n",
    "temp_dict = {}\n",
    "for name in tqdm(df_label.columns[1:]):\n",
    "    print(name)\n",
    "    temp_dict[name] = train_individual_label(df4,  name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-5511764976d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Geeks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geeks'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "my_set = {'Geeks', 'for', 'geeks'} \n",
    "  \n",
    "s = list(my_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cp_type|\n",
      "+-------+\n",
      "| trt_cp|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cp_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'0.8'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "list = [(2147481832,23355149,1),(2147481832,973010692,1),(2147481832,2134870842,1),(2147481832,541023347,1),(2147481832,1682206630,1),(2147481832,1138211459,1),(2147481832,852202566,1),(2147481832,201375938,1),(2147481832,486538879,1),(2147481832,919187908,1),(214748183,919187908,1),(214748183,91187908,1)]\n",
    "df = spark.createDataFrame(list, [\"x1\",\"x2\",\"x3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x1: bigint, x2: bigint, x3: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2147481832: 0.8, 214748183: 0.8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|        x1|fraction|\n",
      "+----------+--------+\n",
      "|2147481832|     0.8|\n",
      "| 214748183|     0.8|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"x1\").distinct().withColumn(\"fraction\", lit(0.8)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = features_and_targets.withColumn('target_vector', (vector_to_string(array([features_and_targets[col] for col in target_names])))).select(['sig_id', 'target_vector'])\n",
    "string_indexer = StringIndexer(inputCol = 'target_vector', outputCol = 'target')\n",
    "string_indexer_model = string_indexer.fit(temp_df)\n",
    "temp_df = string_indexer_model.transform(temp_df).drop('target_vector')\n",
    "\n",
    "data = features_and_targets.join(temp_df, features_and_targets.sig_id == temp_df.sig_id, how = 'inner').drop(temp_df.sig_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
