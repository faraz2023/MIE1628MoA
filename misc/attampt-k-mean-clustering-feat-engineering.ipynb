{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main contributor: Jeff Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "### This part is to run pyspark locally\n",
    "import findspark  # Get rid of this in DataBricks\n",
    "# findspark.init('/opt/spark-3.0.1')  # Get rid of this in DataBricks #faraz: you can remove the parameter. it only worked like this for me\n",
    "findspark.init()\n",
    "########################################\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev, log, log10, sqrt\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from scv import StratifiedCrossValidator\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = spark.read.csv('train_feats.csv', header='true', inferSchema= 'true')   # path in HDFS file system\n",
    "df_label = spark.read.csv('train_targets_scored.csv', header='true', inferSchema= 'true')\n",
    "df = df_train.join(df_label, on=['sig_id'], how='left_outer')  # Jjoin them together\n",
    "df = df.filter(df.cp_type == 'trt_cp')\n",
    "df = df.drop('cp_type')\n",
    "\n",
    "# String indexer for cp_dose\n",
    "indexer = StringIndexer(inputCol=\"cp_dose\", outputCol=\"cp_dose_cat\")\n",
    "# index cp_dose in data\n",
    "df1 = indexer.fit(df).transform(df)\n",
    "\n",
    "# String indexer for cp_time\n",
    "indexer = StringIndexer(inputCol=\"cp_time\", outputCol=\"cp_time_cat\")\n",
    "# index cp_time in data\n",
    "df1 = indexer.fit(df1).transform(df1)\n",
    "df1 = df1.drop('cp_dose')\n",
    "df1 = df1.drop('cp_time')\n",
    "\n",
    "# One-hot enocder \n",
    "encoder = OneHotEncoder(inputCols=[\"cp_time_cat\", \"cp_dose_cat\"],\n",
    "                        outputCols=[\"cp_time_onehot\", \"cp_dose_onehot\"])\n",
    "\n",
    "model = encoder.fit(df1)\n",
    "df1 = model.transform(df1)\n",
    "\n",
    "# Keeping n-1 dummy variables for each feature. (dummy variables have degree of fredom n-1)\n",
    "df1 = df1.withColumn(\"cp_time_cols\", vector_to_array(\"cp_time_onehot\")).select(df1.columns + [col(\"cp_time_cols\")[i] for i in range(2)])\n",
    "df1 = df1.withColumn(\"cp_dose_cols\", vector_to_array(\"cp_dose_onehot\")).select(df1.columns + [col(\"cp_dose_cols\")[i] for i in range(1)])\n",
    "\n",
    "# drop leftover cols\n",
    "df1 = df1.drop('cp_dose_cat',\n",
    " 'cp_time_cat',\n",
    " 'cp_time_onehot',\n",
    " 'cp_dose_onehot',\n",
    ")\n",
    "\n",
    "\n",
    "df2 = df1.withColumn(\"gene_max\", F.greatest(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"gene_min\", F.least(*gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_max\", F.greatest(*cell_feature_names))\n",
    "df2 = df2.withColumn(\"cell_min\", F.least(*cell_feature_names))\n",
    "\n",
    "\n",
    "df2 = df2.withColumn(\"gene_mean\", reduce(lambda x,y: x+y, (col(x) for x in gene_feature_names)) / len(gene_feature_names))\n",
    "df2 = df2.withColumn(\"cell_mean\", reduce(lambda x,y: x+y, (col(x) for x in cell_feature_names)) / len(cell_feature_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "KMeans clustering - engineer new feature based on cluster results\n",
    "Use K=3 to fit the assembled features\n",
    "'''\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "feature_label_assembler = VectorAssembler(inputCols=df1.columns[1:], outputCol=\"assemebled\")\n",
    "cluster_df = feature_label_assembler.transform(df1)\n",
    "\n",
    "kmeans = KMeans(k=3, featuresCol='assemebled', \n",
    "                predictionCol='clusterClassPrediction', distanceMeasure='euclidean',)\n",
    "\n",
    "model = kmeans.fit(cluster_df)\n",
    "transformed = model.transform(cluster_df).select(\"sig_id\", \"clusterClassPrediction\")\n",
    "\n",
    "df2 = df2.join(transformed, on=['sig_id'], how='inner')\n",
    "df2.select(\"clusterClassPrediction\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
